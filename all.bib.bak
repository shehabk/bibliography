% Encoding: ISO-8859-9

@InProceedings{liu2014feature,
  author       = {Liu, Ping and Zhou, Joey Tianyi and Tsang, Ivor Wai-Hung and Meng, Zibo and Han, Shizhong and Tong, Yan},
  title        = {Feature disentangling machine-a novel approach of feature selection and disentangling in facial expression analysis},
  booktitle    = {European Conference on Computer Vision},
  year         = {2014},
  pages        = {151--166},
  organization = {Springer},
  file         = {:liu2014feature - Feature Disentangling Machine a Novel Approach of Feature Selection and Disentangling in Facial Expression Analysis.pdf:PDF},
  groups       = {MyGroup},
}

@Article{cai2017island,
  author  = {Cai, Jie and Meng, Zibo and Khan, Ahmed Shehab and Li, Ziyuan and Tong, Yan},
  title   = {{Island Loss for Learning Discriminative Features in Facial Expression Recognition}},
  journal = {arXiv preprint arXiv:1710.03144},
  year    = {2017},
  comment = {ITBN},
  file    = {:cai2017island - Island Loss for Learning Discriminative Features in Facial Expression Recognition.pdf:PDF},
  groups  = {MyGroup},
}

@InProceedings{meng2017identity,
  author       = {Meng, Zibo and Liu, Ping and Cai, Jie and Han, Shizhong and Tong, Yan},
  title        = {Identity-aware convolutional neural network for facial expression recognition},
  booktitle    = {Automatic Face \& Gesture Recognition (FG 2017), 2017 12th IEEE International Conference on},
  year         = {2017},
  pages        = {558--565},
  organization = {IEEE},
  file         = {:meng2017identity - Identity Aware Convolutional Neural Network for Facial Expression Recognition.pdf:PDF},
  groups       = {MyGroup},
}

@InProceedings{han2016incremental,
  author    = {Han, Shizhong and Meng, Zibo and Khan, Ahmed-Shehab and Tong, Yan},
  title     = {Incremental boosting convolutional neural network for facial action unit recognition},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2016},
  pages     = {109--117},
  file      = {:han2016incremental - Incremental Boosting Convolutional Neural Network for Facial Action Unit Recognition.pdf:PDF},
  groups    = {MyGroup},
}

@InProceedings{chen2016infogan,
  author    = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  title     = {Infogan: Interpretable representation learning by information maximizing generative adversarial nets},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2016},
  pages     = {2172--2180},
  file      = {:chen2016infogan_ - Infogan_ Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.pdf:PDF},
  groups    = {Conditional Adversarial},
}

@Article{tran2017representation,
  author   = {Tran, Luan and Yin, Xi and Liu, Xiaoming},
  title    = {Representation learning by rotating your faces},
  journal  = {arXiv preprint arXiv:1705.11136},
  year     = {2017},
  file     = {:tran2017representation - Representation Learning by Rotating Your Faces.pdf:PDF},
  groups   = {Image Translation GAN},
  keywords = {image translation,nips17},
}

@Article{li2018globala,
  author   = {Li, Peipei and Hu, Yibo and Li, Qi and He, Ran and Sun, Zhenan},
  title    = {Global and Local Consistent Age Generative Adversarial Networks},
  journal  = {arXiv preprint arXiv:1801.08390},
  year     = {2018},
  file     = {:li2018global - Global and Local Consistent Age Generative Adversarial Networks.pdf:PDF},
  groups   = {Face Synthesis GAN},
  keywords = {face synthesis},
}

@Article{zhu2017unpaired,
  author    = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
  title     = {Unpaired image-to-image translation using cycle-consistent adversarial networks},
  journal   = {arXiv preprint arXiv:1703.10593},
  year      = {2017},
  file      = {:zhu2017unpaired - Unpaired Image to Image Translation Using Cycle Consistent Adversarial Networks.pdf:PDF},
  groups    = {Image Translation GAN},
  keywords  = {image translation,nips17,cyclegan},
  publisher = {ICCV},
}

@InProceedings{saad2017dyadgan,
  author    = {Saad Khan, Yuchi M and others},
  title     = {DyadGAN: Generating Facial Expressions in Dyadic Interactions},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  year      = {2017},
  pages     = {11--18},
  file      = {:saad2017dyadgan - DyadGAN_ Generating Facial Expressions in Dyadic Interactions.pdf:PDF},
  groups    = {Face Synthesis GAN},
  keywords  = {face synthesis},
}

@InProceedings{kaneko2017generative,
  author    = {Kaneko, Takuhiro and Hiramatsu, Kaoru and Kashino, Kunio},
  title     = {Generative attribute controller with conditional filtered generative adversarial networks},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2017},
  volume    = {2},
  file      = {:kaneko2017generative - Generative Attribute Controller with Conditional Filtered Generative Adversarial Networks.pdf:PDF},
  groups    = {Face Synthesis GAN},
  keywords  = {face synthesis},
}

@Article{isola2017image,
  author   = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
  title    = {Image-to-image translation with conditional adversarial networks},
  journal  = {arXiv preprint},
  year     = {2017},
  comment  = {Pix2Pix},
  file     = {:isola2017image - Image to Image Translation with Conditional Adversarial Networks.pdf:PDF},
  groups   = {Image Translation GAN},
  keywords = {image translation,pix2pix,rank5},
}

@Article{dolhansky2017eye,
  author   = {Dolhansky, Brian and Ferrer, Cristian Canton},
  title    = {Eye In-Painting with Exemplar Generative Adversarial Networks},
  journal  = {arXiv preprint arXiv:1712.03999},
  year     = {2017},
  file     = {:dolhansky2017eye - Eye in Painting with Exemplar Generative Adversarial Networks.pdf:PDF},
  groups   = {Inpainting},
  keywords = {inpainting},
}

@Article{radford2015unsuperviseda,
  author   = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  title    = {Unsupervised representation learning with deep convolutional generative adversarial networks},
  journal  = {arXiv preprint arXiv:1511.06434},
  year     = {2015},
  file     = {:radford2015unsupervised - Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.pdf:PDF},
  groups   = {High Quality Image Generation},
  keywords = {DC-GAN,iclr, rank5},
}

@Article{mirza2014conditional,
  author   = {Mirza, Mehdi and Osindero, Simon},
  title    = {Conditional generative adversarial nets},
  journal  = {arXiv preprint arXiv:1411.1784},
  year     = {2014},
  file     = {:mirza2014conditional - Conditional Generative Adversarial Nets.pdf:PDF},
  groups   = {Theory GAN, Conditional Adversarial},
  keywords = {rank5},
}

@InProceedings{goodfellow2014generative,
  author    = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  title     = {Generative adversarial nets},
  booktitle = {Advances in neural information processing systems},
  year      = {2014},
  pages     = {2672--2680},
  file      = {:goodfellow2014generative - Generative adversarial nets.pdf:PDF},
  groups    = {Theory GAN},
  keywords  = {rank5},
}

@InProceedings{zhao2015joint,
  author       = {Zhao, Kaili and Chu, Wen-Sheng and De la Torre, Fernando and Cohn, Jeffrey F and Zhang, Honggang},
  title        = {Joint patch and multi-label learning for facial action unit detection},
  booktitle    = {Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on},
  year         = {2015},
  pages        = {2207--2216},
  organization = {IEEE},
  file         = {:zhao2015joint - Joint patch and multi-label learning for facial action unit detection.pdf:PDF},
  groups       = {AU},
  keywords     = {JPML},
}

@InProceedings{li2017action,
  author       = {Li, Wei and Abtahi, Farnaz and Zhu, Zhigang},
  title        = {Action unit detection with region adaptation, multi-labeling learning and optimal temporal fusing},
  booktitle    = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year         = {2017},
  pages        = {6766--6775},
  organization = {IEEE},
  file         = {:li2017action - Action unit detection with region adaptation, multi-labeling learning and optimal temporal fusing.pdf:PDF},
  groups       = {AU},
  owner        = {shehabk},
  timestamp    = {2018.02.10},
}

@InProceedings{zhao2016deep,
  author    = {Zhao, Kaili and Chu, Wen-Sheng and Zhang, Honggang},
  title     = {Deep region and multi-label learning for facial action unit detection},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2016},
  pages     = {3391--3399},
  file      = {:zhao2016deep - Deep region and multi-label learning for facial action unit detection.pdf:PDF},
  groups    = {AU},
  keywords  = {DRML},
  owner     = {shehabk},
  timestamp = {2018.02.10},
}

@InProceedings{li2017eac,
  author       = {Li, Wei and Abtahi, Farnaz and Zhu, Zhigang and Yin, Lijun},
  title        = {Eac-net: A region-based deep enhancing and cropping approach for facial action unit detection},
  booktitle    = {Automatic Face \& Gesture Recognition (FG 2017), 2017 12th IEEE International Conference on},
  year         = {2017},
  pages        = {103--110},
  organization = {IEEE},
  comment      = {This paper got recently got accepted in PAMI},
  file         = {:li2017eac - Eac-net_ A region-based deep enhancing and cropping approach for facial action unit detection.pdf:PDF},
  groups       = {AU},
  owner        = {shehabk},
  timestamp    = {2018.02.10},
}

@Article{zhong2015learning,
  author    = {Zhong, Lin and Liu, Qingshan and Yang, Peng and Huang, Junzhou and Metaxas, Dimitris N},
  title     = {Learning multiscale active facial patches for expression analysis},
  journal   = {IEEE transactions on cybernetics},
  year      = {2015},
  volume    = {45},
  number    = {8},
  pages     = {1499--1510},
  file      = {:zhong2015learning - Learning multiscale active facial patches for expression analysis.pdf:PDF},
  groups    = {AU},
  owner     = {shehabk},
  publisher = {IEEE},
  timestamp = {2018.02.10},
}

@InProceedings{chang2009learning,
  author       = {Chang, Kai-Yueh and Liu, Tyng-Luh and Lai, Shang-Hong},
  title        = {Learning partially-observed hidden conditional random fields for facial expression recognition},
  booktitle    = {Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on},
  year         = {2009},
  pages        = {533--540},
  organization = {IEEE},
  file         = {:chang2009learning - Learning partially-observed hidden conditional random fields for facial expression recognition.pdf:PDF},
  groups       = {AU},
  owner        = {shehabk},
  timestamp    = {2018.02.10},
}

@InProceedings{chu2017learning,
  author       = {Chu, Wen-Sheng and De la Torre, Fernando and Cohn, Jeffrey F},
  title        = {Learning spatial and temporal cues for multi-label facial action unit detection},
  booktitle    = {Automatic Face \& Gesture Recognition (FG 2017), 2017 12th IEEE International Conference on},
  year         = {2017},
  pages        = {25--32},
  organization = {IEEE},
  file         = {:chu2017learning - Learning spatial and temporal cues for multi-label facial action unit detection.pdf:PDF},
  groups       = {AU},
  owner        = {shehabk},
  timestamp    = {2018.02.10},
}

@InProceedings{eleftheriadis2015multi,
  author    = {Eleftheriadis, Stefanos and Rudovic, Ognjen and Pantic, Maja},
  title     = {Multi-conditional latent variable model for joint facial action unit detection},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  year      = {2015},
  pages     = {3792--3800},
  file      = {:eleftheriadis2015multi - Multi-conditional latent variable model for joint facial action unit detection.pdf:PDF},
  groups    = {AU},
  owner     = {shehabk},
  timestamp = {2018.02.10},
}

@InProceedings{yang2014personalized,
  author       = {Yang, Shuang and Rudovic, Ognjen and Pavlovic, Vladimir and Pantic, Maja},
  title        = {Personalized modeling of facial action unit intensity},
  booktitle    = {International Symposium on Visual Computing},
  year         = {2014},
  pages        = {269--281},
  organization = {Springer},
  groups       = {AU},
  owner        = {shehabk},
  timestamp    = {2018.02.10},
}

@InProceedings{chu2013selective,
  author       = {Chu, Wen-Sheng and De la Torre, Fernando and Cohn, Jeffery F},
  title        = {Selective transfer machine for personalized facial action unit detection},
  booktitle    = {Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on},
  year         = {2013},
  pages        = {3515--3522},
  organization = {IEEE},
  file         = {:chu2013selective - Selective transfer machine for personalized facial action unit detection.pdf:PDF},
  groups       = {AU},
  owner        = {shehabk},
  timestamp    = {2018.02.10},
}

@Article{pavlovicdeep,
  author    = {Pavlovic, Vladimir and Schuller, Bj{\"o}ern and Pantic, Maja},
  title     = {Deep Structured Learning for Facial Action Unit Intensity Estimation},
  file      = {:pavlovicdeep - Deep Structured Learning for Facial Action Unit Intensity Estimation.pdf:PDF},
  groups    = {AU},
  owner     = {shehabk},
  timestamp = {2018.02.10},
}

@InProceedings{sangineto2014we,
  author       = {Sangineto, Enver and Zen, Gloria and Ricci, Elisa and Sebe, Nicu},
  title        = {We are not all equal: Personalizing models for facial expression analysis with transductive parameter transfer},
  booktitle    = {Proceedings of the 22nd ACM international conference on Multimedia},
  year         = {2014},
  pages        = {357--366},
  organization = {ACM},
  file         = {:sangineto2014we - We are not all equal_ Personalizing models for facial expression analysis with transductive parameter transfer.pdf:PDF},
  groups       = {AU},
  owner        = {shehabk},
  timestamp    = {2018.02.10},
}

@InProceedings{ding2017facenet2expnet,
  author       = {Ding, Hui and Zhou, Shaohua Kevin and Chellappa, Rama},
  title        = {Facenet2expnet: Regularizing a deep face recognition net for expression recognition},
  booktitle    = {Automatic Face \& Gesture Recognition (FG 2017), 2017 12th IEEE International Conference on},
  year         = {2017},
  pages        = {118--126},
  organization = {IEEE},
  comment      = {person-independent},
  file         = {:ding2017facenet2expnet - Facenet2expnet_ Regularizing a deep face recognition net for expression recognition.pdf:PDF},
  groups       = {Expression},
  owner        = {shehabk},
  timestamp    = {2018.02.10},
}

@InProceedings{zhao2016peak,
  author       = {Zhao, Xiangyun and Liang, Xiaodan and Liu, Luoqi and Li, Teng and Han, Yugang and Vasconcelos, Nuno and Yan, Shuicheng},
  title        = {Peak-piloted deep network for facial expression recognition},
  booktitle    = {European conference on computer vision},
  year         = {2016},
  pages        = {425--442},
  organization = {Springer},
  comment      = {person-independent, PPDN},
  file         = {:zhao2016peak - Peak-piloted deep network for facial expression recognition.pdf:PDF},
  groups       = {Expression},
  owner        = {shehabk},
  timestamp    = {2018.02.10},
}

@Article{zhao2007dynamic,
  author    = {Zhao, Guoying and Pietikainen, Matti},
  title     = {Dynamic texture recognition using local binary patterns with an application to facial expressions},
  journal   = {IEEE transactions on pattern analysis and machine intelligence},
  year      = {2007},
  volume    = {29},
  number    = {6},
  pages     = {915--928},
  comment   = {LBP-TOP},
  file      = {:zhao2007dynamic - Dynamic texture recognition using local binary patterns with an application to facial expressions.pdf:PDF},
  groups    = {Expression},
  owner     = {shehabk},
  publisher = {IEEE},
  timestamp = {2018.02.10},
}

@InProceedings{klaser2008spatio,
  author       = {Klaser, Alexander and Marsza{\l}ek, Marcin and Schmid, Cordelia},
  title        = {A spatio-temporal descriptor based on 3d-gradients},
  booktitle    = {BMVC 2008-19th British Machine Vision Conference},
  year         = {2008},
  pages        = {275--1},
  organization = {British Machine Vision Association},
  comment      = {HOG-3D},
  file         = {:klaser2008spatio - A spatio-temporal descriptor based on 3d-gradients.pdf:PDF},
  groups       = {Expression},
  owner        = {shehabk},
  timestamp    = {2018.02.10},
}

@InProceedings{liu2014deeply,
  author       = {Liu, Mengyi and Li, Shaoxin and Shan, Shiguang and Wang, Ruiping and Chen, Xilin},
  title        = {Deeply learning deformable facial action parts model for dynamic expression analysis},
  booktitle    = {Asian conference on computer vision},
  year         = {2014},
  pages        = {143--157},
  organization = {Springer},
  comment      = {3DCNN-DAP},
  file         = {:liu2014deeply - Deeply learning deformable facial action parts model for dynamic expression analysis.pdf:PDF},
  groups       = {Expression},
  owner        = {shehabk},
  timestamp    = {2018.02.10},
}

@InProceedings{liu2014learning,
  author       = {Liu, Mengyi and Shan, Shiguang and Wang, Ruiping and Chen, Xilin},
  title        = {Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition},
  booktitle    = {Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on},
  year         = {2014},
  pages        = {1749--1756},
  organization = {IEEE},
  comment      = {STM-Explet},
  file         = {:liu2014learning - Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition.pdf:PDF},
  groups       = {Expression},
  owner        = {shehabk},
  timestamp    = {2018.02.10},
}

@InProceedings{jung2015joint,
  author       = {Jung, Heechul and Lee, Sihaeng and Yim, Junho and Park, Sunjeong and Kim, Junmo},
  title        = {Joint fine-tuning in deep neural networks for facial expression recognition},
  booktitle    = {Computer Vision (ICCV), 2015 IEEE International Conference on},
  year         = {2015},
  pages        = {2983--2991},
  organization = {IEEE},
  comment      = {DTAGN},
  file         = {:jung2015joint - Joint fine-tuning in deep neural networks for facial expression recognition.pdf:PDF},
  groups       = {Expression},
  owner        = {shehabk},
  timestamp    = {2018.02.10},
}

@InProceedings{wang2013capturing,
  author       = {Wang, Ziheng and Wang, Shangfei and Ji, Qiang},
  title        = {Capturing complex spatio-temporal relations among facial muscles for facial expression recognition},
  booktitle    = {Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on},
  year         = {2013},
  pages        = {3422--3429},
  organization = {IEEE},
  file         = {:wang2013capturing - Capturing complex spatio-temporal relations among facial muscles for facial expression recognition.pdf:PDF},
  groups       = {Expression},
  owner        = {shehabk},
  timestamp    = {2018.02.10},
}

@Article{sariyanidi2017learning,
  author    = {Sariyanidi, Evangelos and Gunes, Hatice and Cavallaro, Andrea},
  title     = {Learning bases of activity for facial expression recognition},
  journal   = {IEEE Transactions on Image Processing},
  year      = {2017},
  volume    = {26},
  number    = {4},
  pages     = {1965--1978},
  comment   = {F-Bases},
  file      = {:sariyanidi2017learning - Learning bases of activity for facial expression recognition.pdf:PDF},
  groups    = {Expression},
  owner     = {shehabk},
  publisher = {IEEE},
  timestamp = {2018.02.10},
}

@InProceedings{ptucha2011manifold,
  author       = {Ptucha, Raymond and Tsagkatakis, Grigorios and Savakis, Andreas},
  title        = {Manifold based sparse representation for robust expression recognition without neutral subtraction},
  booktitle    = {Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on},
  year         = {2011},
  pages        = {2136--2143},
  organization = {IEEE},
  comment      = {MSR},
  groups       = {Expression},
  owner        = {shehabk},
  timestamp    = {2018.02.10},
}

@InProceedings{jain2011facial,
  author       = {Jain, Suyog and Hu, Changbo and Aggarwal, Jake K},
  title        = {Facial expression recognition with temporal modeling of shapes},
  booktitle    = {Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on},
  year         = {2011},
  pages        = {1642--1649},
  organization = {IEEE},
  comment      = {TMS},
  file         = {:jain2011facial - Facial expression recognition with temporal modeling of shapes.pdf:PDF},
  groups       = {Expression},
  owner        = {shehabk},
  timestamp    = {2018.02.10},
}

@InProceedings{sanin2013spatio,
  author       = {Sanin, Andres and Sanderson, Conrad and Harandi, Mehrtash T and Lovell, Brian C},
  title        = {Spatio-temporal covariance descriptors for action and gesture recognition},
  booktitle    = {Applications of Computer Vision (WACV), 2013 IEEE Workshop on},
  year         = {2013},
  pages        = {103--110},
  organization = {IEEE},
  comment      = {Cov3D},
  file         = {:sanin2013spatio - Spatio-temporal covariance descriptors for action and gesture recognition.pdf:PDF},
  groups       = {Expression},
  owner        = {shehabk},
  timestamp    = {2018.02.10},
}

@InProceedings{mollahosseini2016going,
  author       = {Mollahosseini, Ali and Chan, David and Mahoor, Mohammad H},
  title        = {Going deeper in facial expression recognition using deep neural networks},
  booktitle    = {Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on},
  year         = {2016},
  pages        = {1--10},
  organization = {IEEE},
  comment      = {Inception},
  file         = {:mollahosseini2016going - Going deeper in facial expression recognition using deep neural networks.pdf:PDF},
  groups       = {Expression},
  owner        = {shehabk},
  timestamp    = {2018.02.10},
}

@InProceedings{sikka2016lomo,
  author    = {Sikka, Karan and Sharma, Gaurav and Bartlett, Marian},
  title     = {Lomo: Latent ordinal model for facial analysis in videos},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2016},
  pages     = {5580--5589},
  comment   = {LOMO},
  file      = {:sikka2016lomo - Lomo_ Latent ordinal model for facial analysis in videos.pdf:PDF},
  groups    = {Expression},
  owner     = {shehabk},
  timestamp = {2018.02.10},
}

@InProceedings{li2017reliable,
  author       = {Li, Shan and Deng, Weihong and Du, JunPing},
  title        = {Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild},
  booktitle    = {Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on},
  year         = {2017},
  pages        = {2584--2593},
  organization = {IEEE},
  comment      = {DLP-CNN},
  file         = {:li2017reliable - Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild.pdf:PDF},
  groups       = {Expression},
  owner        = {shehabk},
  timestamp    = {2018.02.10},
}

@Article{chu2017selective,
  author    = {Chu, Wen-Sheng and De la Torre, Fernando and Cohn, Jeffrey F},
  title     = {Selective transfer machine for personalized facial expression analysis},
  journal   = {IEEE transactions on pattern analysis and machine intelligence},
  year      = {2017},
  volume    = {39},
  number    = {3},
  pages     = {529--545},
  comment   = {STM},
  groups    = {Expression},
  owner     = {shehabk},
  publisher = {IEEE},
  timestamp = {2018.02.10},
}

@Article{ding2017exprgan,
  author   = {Ding, Hui and Sricharan, Kumar and Chellappa, Rama},
  title    = {Exprgan: Facial expression editing with controllable expression intensity},
  journal  = {arXiv preprint arXiv:1709.03842},
  year     = {2017},
  file     = {:ding2017exprgan - Exprgan_ Facial expression editing with controllable expression intensity.pdf:PDF},
  groups   = {Expression, Face Synthesis GAN},
  keywords = {face synthesis},
}

@Article{yeh2016semantic,
  author  = {Yeh, Raymond and Liu, Ziwei and Goldman, Dan B and Agarwala, Aseem},
  title   = {Semantic facial expression editing using autoencoded flow},
  journal = {arXiv preprint arXiv:1611.09961},
  year    = {2016},
  groups  = {Expression},
}

@Misc{ekamn1978facial,
  author    = {Ekamn, P and Friesen, W},
  title     = {Facial action coding system (FACS): manual},
  year      = {1978},
  file      = {:ekamn1978facial - Facial action coding system (FACS)_ manual.pdf:PDF},
  groups    = {MyGroup, AU},
  keywords  = {Manual},
  owner     = {shehabk},
  publisher = {Consulting Psychologists Press Palo Alto},
  timestamp = {2018.02.12},
}

@Article{tong2010unified,
  author    = {Tong, Yan and Chen, Jixu and Ji, Qiang},
  title     = {A unified probabilistic framework for spontaneous facial action modeling and understanding},
  journal   = {IEEE transactions on pattern analysis and machine intelligence},
  year      = {2010},
  volume    = {32},
  number    = {2},
  pages     = {258--273},
  file      = {:tong2010unified - A unified probabilistic framework for spontaneous facial action modeling and understanding.pdf:PDF},
  groups    = {MyGroup},
  owner     = {shehabk},
  publisher = {IEEE},
  timestamp = {2018.02.12},
}

@Article{tong2007facial,
  author    = {Tong, Yan and Liao, Wenhui and Ji, Qiang},
  title     = {Facial action unit recognition by exploiting their dynamic and semantic relationships},
  journal   = {IEEE transactions on pattern analysis and machine intelligence},
  year      = {2007},
  volume    = {29},
  number    = {10},
  file      = {:tong2007facial - Facial action unit recognition by exploiting their dynamic and semantic relationships.pdf:PDF},
  groups    = {MyGroup},
  owner     = {shehabk},
  publisher = {IEEE},
  timestamp = {2018.02.12},
}

@Article{sariyanidi2015automatic,
  author    = {Sariyanidi, Evangelos and Gunes, Hatice and Cavallaro, Andrea},
  title     = {Automatic analysis of facial affect: A survey of registration, representation, and recognition},
  journal   = {IEEE transactions on pattern analysis and machine intelligence},
  year      = {2015},
  volume    = {37},
  number    = {6},
  pages     = {1113--1133},
  file      = {:sariyanidi2015automatic - Automatic analysis of facial affect_ A survey of registration, representation, and recognition.pdf:PDF},
  groups    = {Expression},
  owner     = {shehabk},
  publisher = {IEEE},
  timestamp = {2018.02.12},
}

@Article{song2017geometry,
  author    = {Song, Lingxiao and Lu, Zhihe and He, Ran and Sun, Zhenan and Tan, Tieniu},
  title     = {Geometry Guided Adversarial Facial Expression Synthesis},
  journal   = {arXiv preprint arXiv:1712.03474},
  year      = {2017},
  file      = {:song2017geometry - Geometry Guided Adversarial Facial Expression Synthesis.pdf:PDF},
  groups    = {Face Synthesis GAN},
  keywords  = {face synthesis},
  owner     = {shehabk},
  timestamp = {2018.02.19},
}

@Article{antipov2017face,
  author    = {Antipov, Grigory and Baccouche, Moez and Dugelay, Jean-Luc},
  title     = {Face aging with conditional generative adversarial networks},
  journal   = {arXiv preprint arXiv:1702.01983},
  year      = {2017},
  file      = {:antipov2017face - Face aging with conditional generative adversarial networks.pdf:PDF},
  groups    = {Face Synthesis GAN},
  keywords  = {face synthesis},
  owner     = {shehabk},
  timestamp = {2018.02.19},
}

@Article{di2017gp,
  author    = {Di, Xing and Sindagi, Vishwanath A and Patel, Vishal M},
  title     = {GP-GAN: Gender Preserving GAN for Synthesizing Faces from Landmarks},
  journal   = {arXiv preprint arXiv:1710.00962},
  year      = {2017},
  file      = {:di2017gp - GP-GAN_ Gender Preserving GAN for Synthesizing Faces from Landmarks.pdf:PDF},
  groups    = {Face Synthesis GAN},
  keywords  = {face synthesis},
  owner     = {shehabk},
  timestamp = {2018.02.19},
}

@Article{chang2017chinese,
  author    = {Chang, Jie and Gu, Yujun and Zhang, Ya},
  title     = {Chinese Typeface Transformation with Hierarchical Adversarial Network},
  journal   = {arXiv preprint arXiv:1711.06448},
  year      = {2017},
  file      = {:chang2017chinese - Chinese Typeface Transformation with Hierarchical Adversarial Network.pdf:PDF},
  owner     = {shehabk},
  timestamp = {2018.02.19},
}

@Article{huang2017face,
  author    = {Huang, Zhiwu and Kratzwald, Bernhard and Paudel, Danda Pani and Wu, Jiqing and Van Gool, Luc},
  title     = {Face Translation between Images and Videos using Identity-aware CycleGAN},
  journal   = {arXiv preprint arXiv:1712.00971},
  year      = {2017},
  file      = {:huang2017face - Face Translation between Images and Videos using Identity-aware CycleGAN.pdf:PDF},
  groups    = {Face Synthesis GAN},
  keywords  = {face synthesis},
  owner     = {shehabk},
  timestamp = {2018.02.19},
}

@Article{zhou2017label,
  author    = {Zhou, Hao and Sun, Jin and Yacoob, Yaser and Jacobs, David W},
  title     = {Label Denoising Adversarial Network (LDAN) for Inverse Lighting of Face Images},
  journal   = {arXiv preprint arXiv:1709.01993},
  year      = {2017},
  file      = {:zhou2017label - Label Denoising Adversarial Network (LDAN) for Inverse Lighting of Face Images.pdf:PDF},
  groups    = {Face Synthesis GAN},
  owner     = {shehabk},
  timestamp = {2018.02.19},
}

@Article{huang2017facea,
  author    = {Huang, Rui and Zhang, Shu and Li, Tianyu and He, Ran and others},
  title     = {Beyond face rotation: Global and local perception gan for photorealistic and identity preserving frontal view synthesis},
  journal   = {arXiv preprint arXiv:1704.04086},
  year      = {2017},
  file      = {:huang2017facea - Beyond face rotation_ Global and local perception gan for photorealistic and identity preserving frontal view synthesis.pdf:PDF},
  groups    = {Face Synthesis GAN},
  keywords  = {face synthesis,iccv17},
  owner     = {shehabk},
  timestamp = {2018.02.19},
}

@Book{bishop2006pattern,
  title           = {Pattern Recognition and Machine Learning},
  publisher       = {Springer},
  year            = {2006},
  author          = {Bishop, Christopher M.},
  isbn            = {978-0387-31073-2},
  bib2html_rescat = {General ML},
  file            = {:bishop2006pattern - Pattern Recognition and Machine Learning.pdf:PDF},
  groups          = {Books},
  keywords        = {pr,bishop},
  url             = {http://research.microsoft.com/en-us/um/people/cmbishop/prml/},
}

@Book{Duda01a,
  title     = {Pattern Classification (2nd Ed)},
  publisher = {Wiley},
  year      = {2001},
  author    = {Duda},
  annote    = {SIGNATUR = 785.172},
  file      = {:Duda01a - Pattern Classification (2nd Ed).pdf:PDF},
  groups    = {Books},
  keywords  = {pr,duda},
  place     = {Favoritenstrasse 9/4th Floor/1863},
}

@Book{Goodfellow-et-al-2016,
  title     = {Deep Learning},
  publisher = {MIT Press},
  year      = {2016},
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  note      = {\url{http://www.deeplearningbook.org}},
  file      = {:Goodfellow-et-al-2016 - Deep Learning.pdf:PDF},
  groups    = {Books},
  keywords  = {pr,goodfellow},
}

@Book{murphy2012machine,
  title    = {Machine learning: a probabilistic perspective},
  year     = {2012},
  author   = {Kevin P Murphy},
  address  = {Cambridge, MA},
  file     = {:murphy2012machine - Machine learning_ a probabilistic perspective.pdf:PDF},
  groups   = {Books},
  keywords = {pr,murphy},
}

@InProceedings{patternsolution,
  author   = {Duda},
  title    = {Solution Duda Book},
  file     = {:PCDudaHartStorkSlotions.pdf:PDF},
  groups   = {Books},
  keywords = {pr_sol,duda},
}

@Manual{solution,
  title     = {Solution Bishop Book},
  author    = {Bishop},
  file      = {:PRML_solution_full.pdf:PDF},
  groups    = {Books},
  keywords  = {pr_sol,bishop},
  owner     = {shehabk},
  timestamp = {2018.02.19},
}

@Book{computer,
  title     = {Computer vision_ A modern approach-Prentice Hall (2011)},
  author    = {David A Forsyth},
  file      = {:David A. Forsyth, Jean Ponce-Computer vision_ A modern approach-Prentice Hall (2011).pdf:PDF},
  groups    = {Books},
  keywords  = {cv},
  owner     = {shehabk},
  timestamp = {2018.02.19},
}

@Book{probabilistic,
  title     = {Probabilistic graphical models _ principles and techniques-MIT Press (2009.pdf:PDF},
  author    = {Daphne Koller},
  file      = {:(Adaptive computation and machine learning) Daphne Koller_ Nir Friedman -Probabilistic graphical models _ principles and techniques-MIT Press (2009.pdf:PDF},
  groups    = {Books},
  keywords  = {pgm},
  owner     = {shehabk},
  timestamp = {2018.02.19},
}

@Article{goodfellow2016nips,
  author    = {Goodfellow, Ian},
  title     = {NIPS 2016 tutorial: Generative adversarial networks},
  journal   = {arXiv preprint arXiv:1701.00160},
  year      = {2016},
  file      = {:goodfellow2016nips - NIPS 2016 tutorial_ Generative adversarial networks.pdf:PDF;:goodfellow2016nips - NIPS 2016 tutorial_ Generative adversarial networks_slides.pdf:PDF},
  groups    = {Tutorials GAN},
  keywords  = {tutorial},
  owner     = {shehabk},
  timestamp = {2018.02.19},
}

@Article{goodfellow2014distinguishability,
  author    = {Goodfellow, Ian J},
  title     = {On distinguishability criteria for estimating generative models},
  journal   = {arXiv preprint arXiv:1412.6515},
  year      = {2014},
  file      = {:goodfellow2014distinguishability - On distinguishability criteria for estimating generative models.pdf:PDF},
  groups    = {Theory GAN},
  owner     = {shehabk},
  timestamp = {2018.02.19},
}

@InProceedings{zhao2017dual,
  author    = {Zhao, Jian and Xiong, Lin and Jayashree, Panasonic Karlekar and Li, Jianshu and Zhao, Fang and Wang, Zhecan and Pranata, Panasonic Sugiri and Shen, Panasonic Shengmei and Yan, Shuicheng and Feng, Jiashi},
  title     = {Dual-agent gans for photorealistic and identity preserving profile face synthesis},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2017},
  pages     = {65--75},
  file      = {:zhao2017dual - Dual-agent gans for photorealistic and identity preserving profile face synthesis.pdf:PDF},
  groups    = {Face Synthesis GAN},
  keywords  = {face synthesis,nips17},
  owner     = {shehabk},
  timestamp = {2018-02-20},
}

@InProceedings{gansy,
  author   = {Y LeCun},
  title    = {Energy-Based GANs \& other Adversarial things},
  file     = {:gansy - Y LeCun.pdf:PDF},
  groups   = {Tutorials GAN},
  keywords = {tutorial},
}

@Standard{soumithhow,
  title     = {How to Train a GAN},
  author    = {Soumith},
  url       = {https://github.com/soumith/ganhacks},
  comment   = {12) If you have labels, use them (Say For example, Expression Classification.)},
  groups    = {Tutorials GAN},
  keywords  = {tutorial},
  owner     = {shehabk},
  timestamp = {2018.02.20},
}

@InProceedings{salimans2016improved,
  author    = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  title     = {Improved techniques for training gans},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2016},
  pages     = {2234--2242},
  file      = {:salimans2016improved - Improved techniques for training gans.pdf:PDF},
  groups    = {Tutorials GAN},
  url       = {https://github.com/openai/improved-gan},
}

@InProceedings{boosting2017for,
  title    = {Deep Boosting and of Complementary and Networks for Large-Scale},
  year     = {2017},
  file     = {:boosting2017for - _____For Peer Review Only.pdf:PDF},
  groups   = {paper review},
  keywords = {deep boosting, large-scale visual recognition, learning complexities},
}

@InProceedings{expression2018for,
  title    = {Facial Expression and Recognition with Active and Local Shape},
  year     = {2018},
  file     = {:expression2018for - _____For Peer Review Only.pdf:PDF},
  groups   = {paper review},
  keywords = {Feature representation < I.4.7 Feature Measurement < I.4 Image Processing and Computer Vision < I Computing Methodologie},
}

@InProceedings{shen2017learning,
  author       = {Shen, Wei and Liu, Rujie},
  title        = {Learning residual images for face attribute manipulation},
  booktitle    = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year         = {2017},
  pages        = {1225--1233},
  organization = {IEEE},
  file         = {:shen2017learning - Learning residual images for face attribute manipulation.pdf:PDF},
  groups       = {Face Synthesis GAN},
  keywords     = {face synthesis,cvpr17},
  owner        = {shehabk},
  timestamp    = {2018-02-20},
  url          = {https://github.com/Zhongdao/FaceAttributeManipulation},
}

@Article{brock2016neural,
  author    = {Brock, Andrew and Lim, Theodore and Ritchie, James M and Weston, Nick},
  title     = {Neural photo editing with introspective adversarial networks},
  journal   = {arXiv preprint arXiv:1609.07093},
  year      = {2016},
  file      = {:brock2016neural - Neural photo editing with introspective adversarial networks.pdf:PDF},
  groups    = {Face Synthesis GAN},
  keywords  = {face synthesis,iclr17},
  owner     = {shehabk},
  timestamp = {2018-02-20},
}

@Article{shu2017neural,
  author    = {Shu, Zhixin and Yumer, Ersin and Hadap, Sunil and Sunkavalli, Kalyan and Shechtman, Eli and Samaras, Dimitris},
  title     = {Neural face editing with intrinsic image disentangling},
  journal   = {arXiv preprint arXiv:1704.04131},
  year      = {2017},
  file      = {:shu2017neural - Neural face editing with intrinsic image disentangling.pdf:PDF},
  groups    = {Face Synthesis GAN},
  keywords  = {face synthesis,cvpr17},
  owner     = {shehabk},
  timestamp = {2018-02-20},
}

@Article{zhou2017genegan,
  author    = {Zhou, Shuchang and Xiao, Taihong and Yang, Yi and Feng, Dieqiao and He, Qinyao and He, Weiran},
  title     = {Genegan: Learning object transfiguration and attribute subspace from unpaired data},
  journal   = {arXiv preprint arXiv:1705.04932},
  year      = {2017},
  file      = {:zhou2017genegan - Genegan_ Learning object transfiguration and attribute subspace from unpaired data.pdf:PDF},
  groups    = {Face Synthesis GAN},
  keywords  = {face synthesis,bmvc17},
  owner     = {shehabk},
  timestamp = {2018-02-20},
  url       = {https://github.com/Prinsphield/GeneGAN},
}

@Article{choi2017stargan,
  author    = {Choi, Yunjey and Choi, Minje and Kim, Munyoung and Ha, Jung-Woo and Kim, Sunghun and Choo, Jaegul},
  title     = {StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation},
  journal   = {arXiv preprint arXiv:1711.09020},
  year      = {2017},
  file      = {:choi2017stargan - StarGAN_ Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation.pdf:PDF},
  groups    = {Face Synthesis GAN},
  keywords  = {face synthesis},
  owner     = {shehabk},
  timestamp = {2018-02-20},
  url       = {https://github.com/yunjey/StarGAN},
}

@Article{perarnau2016invertible,
  author    = {Perarnau, Guim and van de Weijer, Joost and Raducanu, Bogdan and {\'A}lvarez, Jose M},
  title     = {Invertible Conditional GANs for image editing.(2016)},
  journal   = {arXiv preprint arXiv:1611.06355},
  year      = {2016},
  file      = {:perarnau2016invertible - Invertible Conditional GANs for image editing.(2016).pdf:PDF},
  groups    = {Face Synthesis GAN},
  keywords  = {face synthesis},
  owner     = {shehabk},
  timestamp = {2018-02-20},
  url       = {https://github.com/Guim3/IcGAN},
}

@InProceedings{liu2016coupled,
  author    = {Liu, Ming-Yu and Tuzel, Oncel},
  title     = {Coupled generative adversarial networks},
  booktitle = {Advances in neural information processing systems},
  year      = {2016},
  pages     = {469--477},
  file      = {:liu2016coupled - Coupled generative adversarial networks.pdf:PDF},
  groups    = {Face Synthesis GAN},
  keywords  = {face synthesis,image translation,nips17},
  owner     = {shehabk},
  timestamp = {2018-02-20},
  url       = {https://github.com/andrewliao11/CoGAN-tensorflow},
}

@Article{boesenlindbolarsen2015autoencoding,
  author    = {Boesen Lindbo Larsen, Anders and Kaae S{\o}nderby, S{\o}ren and Larochelle, Hugo and Winther, Ole},
  title     = {Autoencoding beyond pixels using a learned similarity metric},
  journal   = {arXiv preprint arXiv:1512.09300},
  year      = {2015},
  file      = {:boesenlindbolarsen2015autoencoding - Autoencoding beyond pixels using a learned similarity metric.pdf:PDF},
  groups    = {Face Synthesis GAN},
  keywords  = {face synthesis},
  owner     = {shehabk},
  timestamp = {2018-02-20},
  url       = {https://github.com/zhangqianhui/vae-gan-tensorflow},
}

@Article{kim2017learning,
  author    = {Kim, Taeksoo and Cha, Moonsu and Kim, Hyunsoo and Lee, Jungkwon and Kim, Jiwon},
  title     = {Learning to discover cross-domain relations with generative adversarial networks},
  journal   = {arXiv preprint arXiv:1703.05192},
  year      = {2017},
  file      = {:kim2017learning - Learning to discover cross-domain relations with generative adversarial networks.pdf:PDF},
  groups    = {Image Translation GAN},
  keywords  = {image translation,DISCO-GAN},
  owner     = {shehabk},
  timestamp = {2018-02-20},
  url       = {https://github.com/carpedm20/DiscoGAN-pytorch},
}

@InProceedings{liu2017unsupervised,
  author    = {Liu, Ming-Yu and Breuel, Thomas and Kautz, Jan},
  title     = {Unsupervised image-to-image translation networks},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2017},
  pages     = {700--708},
  file      = {:liu2017unsupervised - Unsupervised image-to-image translation networks.pdf:PDF},
  groups    = {Image Translation GAN},
  keywords  = {image translation,nips17},
  owner     = {shehabk},
  timestamp = {2018-02-20},
  url       = {https://github.com/mingyuliutw/UNIT},
}

@InProceedings{gan2017triangle,
  author    = {Gan, Zhe and Chen, Liqun and Wang, Weiyao and Pu, Yuchen and Zhang, Yizhe and Liu, Hao and Li, Chunyuan and Carin, Lawrence},
  title     = {Triangle generative adversarial networks},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2017},
  pages     = {5253--5262},
  file      = {:gan2017triangle - Triangle generative adversarial networks.pdf:PDF},
  groups    = {Image Translation GAN},
  keywords  = {image translation,nips17},
  owner     = {shehabk},
  timestamp = {2018-02-20},
}

@InProceedings{zhang2017st,
  author    = {Zhang, Jichao and Zhong, Fan and Cao, Gongze and Qin, Xueying},
  title     = {ST-GAN: Unsupervised Facial Image Semantic Transformation Using Generative Adversarial Networks},
  booktitle = {Asian Conference on Machine Learning},
  year      = {2017},
  pages     = {248--263},
  file      = {:zhang2017st - ST-GAN_ Unsupervised Facial Image Semantic Transformation Using Generative Adversarial Networks.pdf:PDF},
  groups    = {Image Translation GAN},
  keywords  = {image translation,acml17},
  owner     = {shehabk},
  timestamp = {2018-02-20},
}

@Article{wang2017high,
  author    = {Wang, Ting-Chun and Liu, Ming-Yu and Zhu, Jun-Yan and Tao, Andrew and Kautz, Jan and Catanzaro, Bryan},
  title     = {High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs},
  journal   = {arXiv preprint arXiv:1711.11585},
  year      = {2017},
  file      = {:wang2017high - High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs.pdf:PDF},
  groups    = {Image Translation GAN},
  keywords  = {image translation,pix2pixHD,nvidia},
  owner     = {shehabk},
  timestamp = {2018-02-20},
  url       = {https://github.com/NVIDIA/pix2pixHD},
}

@Article{royer2017xgan,
  author    = {Royer, Am{\'e}lie and Bousmalis, Konstantinos and Gouws, Stephan and Bertsch, Fred and Moressi, Inbar and Cole, Forrester and Murphy, Kevin},
  title     = {XGAN: Unsupervised Image-to-Image Translation for many-to-many Mappings},
  journal   = {arXiv preprint arXiv:1711.05139},
  year      = {2017},
  file      = {:royer2017xgan - XGAN_ Unsupervised Image-to-Image Translation for many-to-many Mappings.pdf:PDF},
  groups    = {Image Translation GAN},
  keywords  = {image translation},
  owner     = {shehabk},
  timestamp = {2018-02-20},
}

@InProceedings{zhu2017multimodal,
  author    = {Zhu, Jun-Yan and Zhang, Richard and Pathak, Deepak and Darrell, Trevor and Efros, Alexei A and Wang, Oliver and Shechtman, Eli},
  title     = {Toward multimodal image-to-image translation},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2017},
  pages     = {465--476},
  file      = {:zhu2017multimodal - Toward multimodal image-to-image translation.pdf:PDF},
  groups    = {Image Translation GAN},
  keywords  = {image translation,nips17,berkeley},
  owner     = {shehabk},
  timestamp = {2018-02-20},
}

@InProceedings{zhang2017stackgan,
  author    = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Huang, Xiaolei and Wang, Xiaogang and Metaxas, Dimitris},
  title     = {Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks},
  booktitle = {IEEE Int. Conf. Comput. Vision (ICCV)},
  year      = {2017},
  pages     = {5907--5915},
  file      = {:zhang2017stackgan - Stackgan_ Text to photo-realistic image synthesis with stacked generative adversarial networks.pdf:PDF},
  groups    = {High Quality Image Generation},
  keywords  = {high quality,iccv17},
  owner     = {shehabk},
  timestamp = {2018-02-20},
}

@InProceedings{tolstikhin2017adagan,
  author    = {Tolstikhin, Ilya O and Gelly, Sylvain and Bousquet, Olivier and Simon-Gabriel, Carl-Johann and Sch{\"o}lkopf, Bernhard},
  title     = {Adagan: Boosting generative models},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2017},
  pages     = {5430--5439},
  groups    = {Unlclassified},
  keywords  = {ensemble,nips17,google brain},
  owner     = {shehabk},
  timestamp = {2018-02-20},
}

@Article{yeh2016semantica,
  author    = {Yeh, Raymond and Chen, Chen and Lim, Teck Yian and Hasegawa-Johnson, Mark and Do, Minh N},
  title     = {Semantic image inpainting with perceptual and contextual losses},
  journal   = {arXiv preprint arXiv:1607.07539},
  year      = {2016},
  file      = {:yeh2016semantic - Semantic image inpainting with perceptual and contextual losses.pdf:PDF},
  groups    = {Inpainting},
  keywords  = {inpainting,cvpr17},
  owner     = {shehabk},
  timestamp = {2018-02-20},
}

@InProceedings{pathak2016context,
  author    = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A},
  title     = {Context encoders: Feature learning by inpainting},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2016},
  pages     = {2536--2544},
  file      = {:pathak2016context - Context encoders_ Feature learning by inpainting.pdf:PDF},
  groups    = {Inpainting},
  keywords  = {inpainting,iccv17},
  owner     = {shehabk},
  timestamp = {2018-02-20},
}

@Article{denton2016semi,
  author    = {Denton, Emily and Gross, Sam and Fergus, Rob},
  title     = {Semi-supervised learning with context-conditional generative adversarial networks},
  journal   = {arXiv preprint arXiv:1611.06430},
  year      = {2016},
  file      = {:denton2016semi - Semi-supervised learning with context-conditional generative adversarial networks.pdf:PDF},
  groups    = {Inpainting},
  keywords  = {inpainting},
  owner     = {shehabk},
  timestamp = {2018-02-20},
}

@InProceedings{li2017generative,
  author    = {Li, Yijun and Liu, Sifei and Yang, Jimei and Yang, Ming-Hsuan},
  title     = {Generative face completion},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2017},
  volume    = {1},
  number    = {3},
  pages     = {6},
  file      = {:li2017generative - Generative face completion.pdf:PDF},
  groups    = {Inpainting},
  keywords  = {inpainting},
  owner     = {shehabk},
  timestamp = {2018-02-20},
}

@Article{iizuka2017globally,
  author    = {Iizuka, Satoshi and Simo-Serra, Edgar and Ishikawa, Hiroshi},
  title     = {Globally and locally consistent image completion},
  journal   = {ACM Transactions on Graphics (TOG)},
  year      = {2017},
  volume    = {36},
  number    = {4},
  pages     = {107},
  file      = {:iizuka2017globally - Globally and locally consistent image completion.pdf:PDF},
  groups    = {Inpainting},
  keywords  = {inpainting},
  owner     = {shehabk},
  publisher = {ACM},
  timestamp = {2018-02-20},
}

@InProceedings{sabour2017dynamic,
  author    = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
  title     = {Dynamic routing between capsules},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2017},
  pages     = {3859--3869},
  comment   = {Here are some good reads:
1) https://medium.com/ai%C2%B3-theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b
2) https://jhui.github.io/2017/11/03/Dynamic-Routing-Between-Capsules/},
  file      = {:sabour2017dynamic - Dynamic routing between capsules.pdf:PDF},
  groups    = {DL Advancement},
  keywords  = {CapsNet,nips17},
  owner     = {shehabk},
  timestamp = {2018.02.22},
}

@Article{hinton2018matrix,
  author    = {Hinton, Geoffrey and Frosst, Nicholas and Sabour, Sara},
  title     = {Matrix capsules with EM routing},
  year      = {2018},
  comment   = {Good Reading:
1) https://jhui.github.io/2017/11/14/Matrix-Capsules-with-EM-routing-Capsule-Network/},
  file      = {:hinton2018matrix - Matrix capsules with EM routing.pdf:PDF},
  groups    = {DL Advancement},
  owner     = {shehabk},
  timestamp = {2018.02.22},
}

@InProceedings{strangintroduction,
  author   = {Gilbert Strang},
  title    = {Introduction to Linear Algebra Fourth Edition},
  file     = {:strangintroduction - Introduction to Linear Algebra Fourth Edition.pdf:PDF},
  groups   = {Books},
  keywords = {linear algebra},
}

@InProceedings{brandt2012matrix,
  author   = {Kaare Brandt and Petersen and Michael Syskind and Pedersen},
  title    = {The Matrix Cookbook},
  year     = {2012},
  file     = {:brandt2012matrix - The Matrix Cookbook [ http___matrixcookbook.com.pdf:PDF},
  groups   = {Books},
  keywords = {matrix,cookbook},
}

@InProceedings{terejanu2015csce883,
  author = {Gabriel Terejanu},
  title  = {CSCE883: Machine Learning},
  year   = {2015},
  file   = {:terejanu2015csce883 - CSCE883_ Machine Learning.pdf:PDF},
  groups = {Course Slides},
}

@InProceedings{tong2015csce,
  author = {Yan Tong},
  title  = {CSCE 867: Computer Vision},
  year   = {2015},
  file   = {:tong2015csce - CSCE 867_ Computer Vision.pdf:PDF},
  groups = {Course Slides},
}

@InProceedings{tong2016csce,
  author = {Yan Tong},
  title  = {CSCE 763: Digital Image Processing},
  year   = {2016},
  file   = {:tong2016csce - CSCE 763_ Digital Image Processing.pdf:PDF},
  groups = {Course Slides},
}

@InProceedings{kollercourserapgm,
  author   = {Daphne Koller},
  title    = {Coursera:PGM Slides},
  file     = {:kollercourserapgm - Coursera_PGM Slides.pdf:PDF},
  groups   = {Course Slides},
  keywords = {manual},
}

@InProceedings{larochelleneural,
  author = {Hugo Larochelle},
  title  = {Neural Networks: Hugo Larochelle},
  file   = {:larochelleneural - Neural Networks_ Hugo Larochelle.pdf:PDF},
  groups = {Course Slides},
}

@InProceedings{li2017cs231,
  author  = {Fei-Fei Li and \& Justin and Johnson and Serena Yeung},
  title   = {CS231: Spring17},
  year    = {2017},
  comment = {Good Reads:
https://www.slideshare.net/TessFerrandez/notes-from-coursera-deep-learning-courses-by-andrew-ng},
  file    = {:li2017cs231 - CS231_ Spring17.pdf:PDF},
  groups  = {Course Slides},
}

@InProceedings{rekleitisrobotics,
  author = {Ioannis Rekleitis},
  title  = {Robotics 574},
  file   = {:rekleitisrobotics - Robotics 574.pdf:PDF},
  groups = {Course Slides},
}

@Standard{theory,
  title     = {Theory 551},
  author    = {Fenner},
  file      = {:default.pdf:PDF},
  groups    = {Course Slides},
  keywords  = {class note},
  owner     = {shehabk},
  timestamp = {2018.02.23},
}

@Book{schapireboosting,
  title  = {Boosting: Foundations and Algorithms},
  author = {Robert E. Schapire, Yoav Freund},
  file   = {:schapireboosting - Boosting_ Foundations and Algorithms.pdf:PDF},
  md5    = {99d47ba2992cf52e799d6fb412f02d1d},
  resid  = {919910},
  size   = {5522793},
  status = {OK},
  type   = {pdf},
}

@InProceedings{systemfacial,
  author   = {System},
  title    = {Facial Action Coding},
  file     = {:systemfacial - Facial Action Coding.pdf:PDF},
  groups   = {AU},
  keywords = {InvGuide},
}

@InProceedings{dhall2016emotiw,
  author       = {Dhall, Abhinav and Goecke, Roland and Joshi, Jyoti and Hoey, Jesse and Gedeon, Tom},
  title        = {Emotiw 2016: Video and group-level emotion recognition challenges},
  booktitle    = {Proceedings of the 18th ACM International Conference on Multimodal Interaction},
  year         = {2016},
  pages        = {427--432},
  organization = {ACM},
  file         = {:dhall2016emotiw - Emotiw 2016_ Video and group-level emotion recognition challenges.pdf:PDF},
  groups       = {EmotiW},
  keywords     = {baseline},
  owner        = {shehabk},
  timestamp    = {2018.03.19},
}

@InProceedings{dhall2015video,
  author       = {Dhall, Abhinav and Ramana Murthy, OV and Goecke, Roland and Joshi, Jyoti and Gedeon, Tom},
  title        = {Video and image based emotion recognition challenges in the wild: Emotiw 2015},
  booktitle    = {Proceedings of the 2015 ACM on International Conference on Multimodal Interaction},
  year         = {2015},
  pages        = {423--426},
  organization = {ACM},
  file         = {:dhall2015video - Video and image based emotion recognition challenges in the wild_ Emotiw 2015.pdf:PDF},
  groups       = {EmotiW},
  keywords     = {baseline},
  owner        = {shehabk},
  timestamp    = {2018.03.19},
}

@InProceedings{dhall2014emotion,
  author       = {Dhall, Abhinav and Goecke, Roland and Joshi, Jyoti and Sikka, Karan and Gedeon, Tom},
  title        = {Emotion recognition in the wild challenge 2014: Baseline, data and protocol},
  booktitle    = {Proceedings of the 16th International Conference on Multimodal Interaction},
  year         = {2014},
  pages        = {461--466},
  organization = {ACM},
  file         = {:dhall2014emotion - Emotion recognition in the wild challenge 2014_ Baseline, data and protocol.pdf:PDF},
  groups       = {EmotiW},
  keywords     = {baseline},
  owner        = {shehabk},
  timestamp    = {2018.03.19},
}

@InProceedings{dhall2013emotion,
  author       = {Dhall, Abhinav and Goecke, Roland and Joshi, Jyoti and Wagner, Michael and Gedeon, Tom},
  title        = {Emotion recognition in the wild challenge 2013},
  booktitle    = {Proceedings of the 15th ACM on International conference on multimodal interaction},
  year         = {2013},
  pages        = {509--516},
  organization = {ACM},
  file         = {:dhall2013emotion - Emotion recognition in the wild challenge 2013.pdf:PDF},
  groups       = {EmotiW},
  keywords     = {baseline},
  owner        = {shehabk},
  timestamp    = {2018.03.19},
}

@InProceedings{yao2015capturing,
  author       = {Yao, Anbang and Shao, Junchao and Ma, Ningning and Chen, Yurong},
  title        = {Capturing au-aware facial features and their latent relations for emotion recognition in the wild},
  booktitle    = {Proceedings of the 2015 ACM on International Conference on Multimodal Interaction},
  year         = {2015},
  pages        = {451--458},
  organization = {ACM},
  file         = {:yao2015capturing - Capturing au-aware facial features and their latent relations for emotion recognition in the wild.pdf:PDF},
  groups       = {EmotiW},
  keywords     = {winner,video based},
  owner        = {shehabk},
  timestamp    = {2018.03.19},
}

@InProceedings{kim2015hierarchical,
  author       = {Kim, Bo-Kyeong and Lee, Hwaran and Roh, Jihyeon and Lee, Soo-Young},
  title        = {Hierarchical committee of deep cnns with exponentially-weighted decision fusion for static facial expression recognition},
  booktitle    = {Proceedings of the 2015 ACM on International Conference on Multimodal Interaction},
  year         = {2015},
  pages        = {427--434},
  organization = {ACM},
  file         = {:kim2015hierarchical - Hierarchical committee of deep cnns with exponentially-weighted decision fusion for static facial expression recognition.pdf:PDF},
  groups       = {EmotiW},
  keywords     = {winner,image based},
  owner        = {shehabk},
  timestamp    = {2018.03.19},
}

@InProceedings{liu2014combining,
  author       = {Liu, Mengyi and Wang, Ruiping and Li, Shaoxin and Shan, Shiguang and Huang, Zhiwu and Chen, Xilin},
  title        = {Combining multiple kernel methods on riemannian manifold for emotion recognition in the wild},
  booktitle    = {Proceedings of the 16th International Conference on Multimodal Interaction},
  year         = {2014},
  pages        = {494--501},
  organization = {ACM},
  file         = {:liu2014combining - Combining multiple kernel methods on riemannian manifold for emotion recognition in the wild.pdf:PDF},
  groups       = {EmotiW},
  keywords     = {winner},
  owner        = {shehabk},
  timestamp    = {2018.03.19},
}

@InProceedings{dhall2017individual,
  author       = {Dhall, Abhinav and Goecke, Roland and Ghosh, Shreya and Joshi, Jyoti and Hoey, Jesse and Gedeon, Tom},
  title        = {From individual to group-level emotion recognition: EmotiW 5.0},
  booktitle    = {Proceedings of the 19th ACM International Conference on Multimodal Interaction},
  year         = {2017},
  pages        = {524--528},
  organization = {ACM},
  file         = {:dhall2017individual - From individual to group-level emotion recognition_ EmotiW 5.0.pdf:PDF},
  groups       = {EmotiW},
  keywords     = {baseline},
  owner        = {shehabk},
  timestamp    = {2018.03.19},
}

@InProceedings{hu2017learning,
  author       = {Hu, Ping and Cai, Dongqi and Wang, Shandong and Yao, Anbang and Chen, Yurong},
  title        = {Learning supervised scoring ensemble for emotion recognition in the wild},
  booktitle    = {Proceedings of the 19th ACM International Conference on Multimodal Interaction},
  year         = {2017},
  pages        = {553--560},
  organization = {ACM},
  file         = {:hu2017learning - Learning supervised scoring ensemble for emotion recognition in the wild.pdf:PDF},
  groups       = {EmotiW},
  keywords     = {winner, audio-video},
  owner        = {shehabk},
  timestamp    = {2018.03.19},
}

@Article{knyazev2017convolutional,
  author    = {Knyazev, Boris and Shvetsov, Roman and Efremova, Natalia and Kuharenko, Artem},
  title     = {Convolutional neural networks pretrained on large face recognition datasets for emotion classification from video},
  journal   = {arXiv preprint arXiv:1711.04598},
  year      = {2017},
  file      = {:knyazev2017convolutional - Convolutional neural networks pretrained on large face recognition datasets for emotion classification from video.pdf:PDF},
  groups    = {EmotiW},
  keywords  = {runner-up, audio-video},
  owner     = {shehabk},
  timestamp = {2018.03.19},
}

@InProceedings{vielzeuf2017temporal,
  author       = {Vielzeuf, Valentin and Pateux, St{\'e}phane and Jurie, Fr{\'e}d{\'e}ric},
  title        = {Temporal multimodal fusion for video emotion classification in the wild},
  booktitle    = {Proceedings of the 19th ACM International Conference on Multimodal Interaction},
  year         = {2017},
  pages        = {569--576},
  organization = {ACM},
  file         = {:vielzeuf2017temporal - Temporal multimodal fusion for video emotion classification in the wild.pdf:PDF},
  groups       = {EmotiW},
  keywords     = {3rd, audio-video},
  owner        = {shehabk},
  timestamp    = {2018.03.19},
}

@InProceedings{tan2017group,
  author       = {Tan, Lianzhi and Zhang, Kaipeng and Wang, Kai and Zeng, Xiaoxing and Peng, Xiaojiang and Qiao, Yu},
  title        = {Group emotion recognition with individual facial emotion CNNs and global image based CNNs},
  booktitle    = {Proceedings of the 19th ACM International Conference on Multimodal Interaction},
  year         = {2017},
  pages        = {549--552},
  organization = {ACM},
  file         = {:tan2017group - Group emotion recognition with individual facial emotion CNNs and global image based CNNs.pdf:PDF},
  groups       = {EmotiW},
  keywords     = {winner, group-based},
  owner        = {shehabk},
  timestamp    = {2018.03.19},
}

@InProceedings{guo2017group,
  author       = {Guo, Xin and Polan{\'\i}a, Luisa F and Barner, Kenneth E},
  title        = {Group-level emotion recognition using deep models on image scene, faces, and skeletons},
  booktitle    = {Proceedings of the 19th ACM International Conference on Multimodal Interaction},
  year         = {2017},
  pages        = {603--608},
  organization = {ACM},
  file         = {:guo2017group - Group-level emotion recognition using deep models on image scene, faces, and skeletons.pdf:PDF},
  groups       = {EmotiW},
  keywords     = {runner-up,group-based},
  owner        = {shehabk},
  timestamp    = {2018.03.19},
}

@InProceedings{wei2017new,
  author       = {Wei, Qinglan and Zhao, Yijia and Xu, Qihua and Li, Liandong and He, Jun and Yu, Lejun and Sun, Bo},
  title        = {A new deep-learning framework for group emotion recognition},
  booktitle    = {Proceedings of the 19th ACM International Conference on Multimodal Interaction},
  year         = {2017},
  pages        = {587--592},
  organization = {ACM},
  file         = {:wei2017new - A new deep-learning framework for group emotion recognition.pdf:PDF},
  groups       = {EmotiW},
  keywords     = {3rd,group-based},
  owner        = {shehabk},
  timestamp    = {2018.03.19},
}

@InProceedings{li2016happiness,
  author       = {Li, Jianshu and Roy, Sujoy and Feng, Jiashi and Sim, Terence},
  title        = {Happiness level prediction with sequential inputs via multiple regressions},
  booktitle    = {Proceedings of the 18th ACM International Conference on Multimodal Interaction},
  year         = {2016},
  pages        = {487--493},
  organization = {ACM},
  file         = {:li2016happiness - Happiness level prediction with sequential inputs via multiple regressions.pdf:PDF},
  groups       = {EmotiW},
  keywords     = {winner,group-based},
  owner        = {shehabk},
  timestamp    = {2018.03.19},
}

@InProceedings{vonikakis2016group,
  author       = {Vonikakis, Vassilios and Yazici, Yasin and Nguyen, Viet Dung and Winkler, Stefan},
  title        = {Group happiness assessment using geometric features and dataset balancing},
  booktitle    = {Proceedings of the 18th ACM International Conference on Multimodal Interaction},
  year         = {2016},
  pages        = {479--486},
  organization = {ACM},
  file         = {:vonikakis2016group - Group happiness assessment using geometric features and dataset balancing.pdf:PDF},
  groups       = {EmotiW},
  keywords     = {runner-up,grpup-based},
  owner        = {shehabk},
  timestamp    = {2018.03.19},
}

@InProceedings{sun2016lstm,
  author       = {Sun, Bo and Wei, Qinglan and Li, Liandong and Xu, Qihua and He, Jun and Yu, Lejun},
  title        = {LSTM for dynamic emotion and group emotion recognition in the wild},
  booktitle    = {Proceedings of the 18th ACM International Conference on Multimodal Interaction},
  year         = {2016},
  pages        = {451--457},
  organization = {ACM},
  file         = {:sun2016lstm - LSTM for dynamic emotion and group emotion recognition in the wild.pdf:PDF},
  groups       = {EmotiW},
  keywords     = {3rd,group-based},
  owner        = {shehabk},
  timestamp    = {2018.03.19},
}

@InProceedings{fan2016video,
  author       = {Fan, Yin and Lu, Xiangju and Li, Dian and Liu, Yuanliu},
  title        = {Video-based emotion recognition using CNN-RNN and C3D hybrid networks},
  booktitle    = {Proceedings of the 18th ACM International Conference on Multimodal Interaction},
  year         = {2016},
  pages        = {445--450},
  organization = {ACM},
  file         = {:fan2016video - Video-based emotion recognition using CNN-RNN and C3D hybrid networks.pdf:PDF},
  groups       = {EmotiW},
  keywords     = {winner,video-based},
  owner        = {shehabk},
  timestamp    = {2018.03.19},
}

@InProceedings{yao2016holonet,
  author       = {Yao, Anbang and Cai, Dongqi and Hu, Ping and Wang, Shandong and Sha, Liang and Chen, Yurong},
  title        = {HoloNet: towards robust emotion recognition in the wild},
  booktitle    = {Proceedings of the 18th ACM International Conference on Multimodal Interaction},
  year         = {2016},
  pages        = {472--478},
  organization = {ACM},
  file         = {:yao2016holonet - HoloNet_ towards robust emotion recognition in the wild.pdf:PDF},
  groups       = {EmotiW},
  keywords     = {runner-up,video-based},
  owner        = {shehabk},
  timestamp    = {2018.03.19},
}

@InProceedings{bargal2016emotion,
  author       = {Bargal, Sarah Adel and Barsoum, Emad and Ferrer, Cristian Canton and Zhang, Cha},
  title        = {Emotion recognition in the wild from videos using images},
  booktitle    = {Proceedings of the 18th ACM International Conference on Multimodal Interaction},
  year         = {2016},
  pages        = {433--436},
  organization = {ACM},
  file         = {:bargal2016emotion - Emotion recognition in the wild from videos using images.pdf:PDF},
  groups       = {EmotiW},
  keywords     = {3rd,video-based},
  owner        = {shehabk},
  timestamp    = {2018.03.19},
}

@InProceedings{Yan:2016:MFE:2993148.2997630,
  author    = {Yan, Jingwei and Zheng, Wenming and Cui, Zhen and Tang, Chuangao and Zhang, Tong and Zong, Yuan and Sun, Ning},
  title     = {Multi-clue Fusion for Emotion Recognition in the Wild},
  booktitle = {Proceedings of the 18th ACM International Conference on Multimodal Interaction},
  year      = {2016},
  series    = {ICMI 2016},
  pages     = {458--463},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2997630},
  doi       = {10.1145/2993148.2997630},
  file      = {:Yan_2016_MFE_2993148.2997630 - Multi-clue Fusion for Emotion Recognition in the Wild.pdf:PDF},
  groups    = {EmotiW},
  isbn      = {978-1-4503-4556-9},
  keywords  = {3rd,video-based},
  location  = {Tokyo, Japan},
  numpages  = {6},
  owner     = {shehabk},
  timestamp = {2018.03.19},
  url       = {http://doi.acm.org/10.1145/2993148.2997630},
}

@InCollection{rasmussen2004gaussian,
  author    = {Rasmussen, Carl Edward},
  title     = {Gaussian processes in machine learning},
  booktitle = {Advanced lectures on machine learning},
  publisher = {Springer},
  year      = {2004},
  pages     = {63--71},
  file      = {:rasmussen2004gaussian - Gaussian processes in machine learning.pdf:PDF},
  groups    = {Books},
  keywords  = {gaussian processes},
  owner     = {shehabk},
  timestamp = {2018-02-26},
}

@Standard{explaining,
  title     = {Explaining AdaBoost},
  comment   = {Others:
http://www.cs.man.ac.uk/~stapenr5/boosting.pdf (Seems very good)},
  file      = {:explaining - Explaining AdaBoost.pdf:PDF},
  groups    = {Tutorials},
  owner     = {shehabk},
  timestamp = {2018-02-26},
}

@InProceedings{dhall2015more,
  author       = {Dhall, Abhinav and Joshi, Jyoti and Sikka, Karan and Goecke, Roland and Sebe, Nicu},
  title        = {The more the merrier: Analysing the affect of a group of people in images},
  booktitle    = {Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on},
  year         = {2015},
  volume       = {1},
  pages        = {1--8},
  organization = {IEEE},
  file         = {:dhall2015more - The more the merrier_ Analysing the affect of a group of people in images.pdf:PDF},
  groups       = {EmotiW},
  keywords     = {reference,group-based},
  owner        = {shehabk},
  timestamp    = {2018.03.20},
}

@InProceedings{dhall2017individuala,
  author       = {Dhall, Abhinav and Goecke, Roland and Ghosh, Shreya and Joshi, Jyoti and Hoey, Jesse and Gedeon, Tom},
  title        = {From individual to group-level emotion recognition: EmotiW 5.0},
  booktitle    = {Proceedings of the 19th ACM International Conference on Multimodal Interaction},
  year         = {2017},
  pages        = {524--528},
  organization = {ACM},
  file         = {:dhall2017individual - From individual to group-level emotion recognition_ EmotiW 5.0.pdf:PDF},
  groups       = {EmotiW},
  keywords     = {reference,group-based},
  owner        = {shehabk},
  timestamp    = {2018.03.20},
}

@InProceedings{dhall2015morea,
  author   = {Abhinav Dhall and Jyoti Joshi and Karan Sikka and Roland Goecke and Nicu Sebe},
  title    = {The More the Merrier: Analysing the Affect of a Group of People in Images},
  year     = {2015},
  file     = {:dhall2015morea - The More the Merrier_ Analysing the Affect of a Group of People in Images.pdf:PDF},
  groups   = {EmotiW},
  keywords = {reference,group-based,lecture},
}

@InProceedings{rosecsce587,
  author = {Rose},
  title  = {CSCE:587 Big Data},
  file   = {:rosecsce587 - CSCE_587 Big Data.pdf:PDF},
  groups = {Course Slides},
}

@InProceedings{by2015propagated,
  author   = {Youjie Zhou},
  title    = {Propagated Image Segmentation Using Edge-Weighted Centroidal Voronoi Tessellation Based Methods},
  year     = {2015},
  file     = {:by2015propagated - Propagated Image Segmentation Using Edge-Weighted Centroidal Voronoi Tessellation Based Methods.pdf:PDF},
  groups   = {segmentation},
  keywords = {thesis},
}

@InProceedings{by20133d,
  author   = {Yu Cao},
  title    = {3D GRAIN SEGMENTATION IN SUPERALLOY IMAGES USING MULTICHANNEL EDGE-WEIGHTED CENTROIDAL VORONOI TESSELLATION BASED METHODS},
  year     = {2013},
  file     = {:by20133d - 3D GRAIN SEGMENTATION IN SUPERALLOY IMAGES USING MULTICHANNEL EDGE-WEIGHTED CENTROIDAL VORONOI TESSELLATION BASED METHODS.pdf:PDF},
  groups   = {segmentation},
  keywords = {thesis},
}

@Article{garcia2017review,
  author    = {Garcia-Garcia, Alberto and Orts-Escolano, Sergio and Oprea, Sergiu and Villena-Martinez, Victor and Garcia-Rodriguez, Jose},
  title     = {A review on deep learning techniques applied to semantic segmentation},
  journal   = {arXiv preprint arXiv:1704.06857},
  year      = {2017},
  file      = {:garcia2017review - A review on deep learning techniques applied to semantic segmentation.pdf:PDF},
  groups    = {segmentation},
  owner     = {shehabk},
  timestamp = {2018.03.28},
}

@Book{halimcompetative,
  title     = {Competative Programiing 3},
  author    = {Steven Halim},
  file      = {:halimcompetative - Competative Programiing 3.pdf:PDF},
  groups    = {Books},
  owner     = {shehabk},
  timestamp = {2018.04.17},
}

@Article{shao2018deep,
  author    = {Shao, Zhiwen and Liu, Zhilei and Cai, Jianfei and Ma, Lizhuang},
  title     = {Deep Adaptive Attention for Joint Facial Action Unit Detection and Face Alignment},
  journal   = {arXiv preprint arXiv:1803.05588},
  year      = {2018},
  comment   = {This paper uses attention.},
  file      = {:shao2018deep - Deep Adaptive Attention for Joint Facial Action Unit Detection and Face Alignment.pdf:PDF},
  groups    = {AU},
  owner     = {shehabk},
  timestamp = {2018.05.08},
}

@InProceedings{rossum2012python,
  author = {Guido van Rossum and Fred L. Drake and Jr and editor},
  title  = {The Python Language Reference Release},
  year   = {2012},
  file   = {:rossum2012python - The Python Language Reference Release.pdf:PDF},
  groups = {Python},
}

@InProceedings{rossum2012pythona,
  author = {Guido van Rossum and Fred L. Drake and Jr and editor},
  title  = {The Python Library Reference Release},
  year   = {2012},
  file   = {:rossum2012pythona - The Python Library Reference Release.pdf:PDF},
  groups = {Python},
}

@InProceedings{course,
  title  = {Service Oriented Architecture Course Notes},
  file   = {:Service-Oriented-Architecture-Courese-Notes.pdf:PDF},
  groups = {Design Patterns and Software Arhitecture},
}

@InProceedings{coursea,
  title  = {Object Oriented Design Course Notes},
  file   = {:Object-Oriented-Design-Course-Notes.pdf:PDF},
  groups = {Design Patterns and Software Arhitecture},
}

@InProceedings{courseb,
  title  = {Design Pattern Course Notes},
  file   = {:Desigh-Patterns-Course-Notes.pdf:PDF},
  groups = {Design Patterns and Software Arhitecture},
}

@InProceedings{???in??object-oriented??designcourse,
  author = {c1},
  title  = {C1 Glossary},
  file   = {:C1-Glossary.pdf:PDF},
  groups = {Design Patterns and Software Arhitecture},
}

@InProceedings{abstract?superclasscourse,
  author = {c2},
  title  = {C2-Glossary},
  file   = {:C2-Glossary.pdf:PDF},
  groups = {Design Patterns and Software Arhitecture},
}

@InProceedings{aggregation?course,
  author = {c3},
  title  = {C4-Glossary},
  file   = {:C4-Glossary.pdf:PDF},
  groups = {Design Patterns and Software Arhitecture},
}

@Article{tulyakov2017mocogan,
  author    = {Tulyakov, Sergey and Liu, Ming-Yu and Yang, Xiaodong and Kautz, Jan},
  title     = {Mocogan: Decomposing motion and content for video generation},
  journal   = {arXiv preprint arXiv:1707.04993},
  year      = {2017},
  file      = {:tulyakov2017mocogan - Mocogan_ Decomposing motion and content for video generation.pdf:PDF},
  groups    = {Dynamic Gan},
  owner     = {shehabk},
  timestamp = {2018.06.18},
}

@InProceedings{oh2015action,
  author    = {Oh, Junhyuk and Guo, Xiaoxiao and Lee, Honglak and Lewis, Richard L and Singh, Satinder},
  title     = {Action-conditional video prediction using deep networks in atari games},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2015},
  pages     = {2863--2871},
  comment   = {This one is not using Gan for generating images. Using Reinforcement Learning. But very similar in lstm modelling.},
  file      = {:oh2015action - Action-conditional video prediction using deep networks in atari games.pdf:PDF},
  groups    = {Dynamic Gan},
  owner     = {shehabk},
  timestamp = {2018.06.18},
}

@InProceedings{srivastava2015unsupervised,
  author    = {Srivastava, Nitish and Mansimov, Elman and Salakhudinov, Ruslan},
  title     = {Unsupervised learning of video representations using lstms},
  booktitle = {International conference on machine learning},
  year      = {2015},
  pages     = {843--852},
  file      = {:srivastava2015unsupervised - Unsupervised learning of video representations using lstms.pdf:PDF},
  groups    = {Dynamic Gan},
  owner     = {shehabk},
  timestamp = {2018.06.18},
}

@Article{mathieu2015deep,
  author    = {Mathieu, Michael and Couprie, Camille and LeCun, Yann},
  title     = {Deep multi-scale video prediction beyond mean square error},
  journal   = {arXiv preprint arXiv:1511.05440},
  year      = {2015},
  comment   = {This paper do utilizes gan and a multiscale cnn. They introduces the gradient difference loss},
  file      = {:mathieu2015deep - Deep multi-scale video prediction beyond mean square error.pdf:PDF},
  groups    = {Dynamic Gan},
  owner     = {shehabk},
  timestamp = {2018.06.18},
}

@Article{kalchbrenner2016video,
  author    = {Kalchbrenner, Nal and Oord, Aaron van den and Simonyan, Karen and Danihelka, Ivo and Vinyals, Oriol and Graves, Alex and Kavukcuoglu, Koray},
  title     = {Video pixel networks},
  journal   = {arXiv preprint arXiv:1610.00527},
  year      = {2016},
  comment   = {Looks hard. Based on Pixel CNN},
  file      = {:kalchbrenner2016video - Video pixel networks.pdf:PDF},
  groups    = {Dynamic Gan},
  owner     = {shehabk},
  timestamp = {2018.06.18},
}

@InProceedings{finn2016unsupervised,
  author    = {Finn, Chelsea and Goodfellow, Ian and Levine, Sergey},
  title     = {Unsupervised learning for physical interaction through video prediction},
  booktitle = {Advances in neural information processing systems},
  year      = {2016},
  pages     = {64--72},
  comment   = {Has Good Fellow in it. It kind of uses convolutional lstm to model the dynamics in every layer.},
  file      = {:finn2016unsupervised - Unsupervised learning for physical interaction through video prediction.pdf:PDF},
  groups    = {Dynamic Gan},
  owner     = {shehabk},
  timestamp = {2018.06.18},
}

@Article{van2017transformation,
  author    = {Van Amersfoort, Joost and Kannan, Anitha and Ranzato, Marc'Aurelio and Szlam, Arthur and Tran, Du and Chintala, Soumith},
  title     = {Transformation-based models of video sequences},
  journal   = {arXiv preprint arXiv:1701.08435},
  year      = {2017},
  comment   = {This paper tries to find affine transformation for overlapping patches and then somehow combines them to form the frame.},
  file      = {:van2017transformation - Transformation-based models of video sequences.pdf:PDF},
  groups    = {Dynamic Gan},
  owner     = {shehabk},
  timestamp = {2018.06.18},
}

@InProceedings{xue2016probabilistic,
  author    = {Xue, T and Wu, J and Bouman, K and Freeman, B},
  title     = {Probabilistic modeling of future frames from a single image},
  booktitle = {NIPS},
  year      = {2016},
  comment   = {This kind of models the distriibution of possible next frames in some probabilistic way.},
  file      = {:xue2016probabilistic - Probabilistic modeling of future frames from a single image.pdf:PDF},
  groups    = {Dynamic Gan},
  owner     = {shehabk},
  timestamp = {2018.06.18},
}

@InProceedings{denton2017unsupervised,
  author    = {Denton, Emily L and others},
  title     = {Unsupervised learning of disentangled representations from video},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2017},
  pages     = {4417--4426},
  file      = {:denton2017unsupervised - Unsupervised learning of disentangled representations from video.pdf:PDF},
  groups    = {Dynamic Gan},
  owner     = {shehabk},
  timestamp = {2018.06.18},
}

@Article{villegas2017decomposing,
  author    = {Villegas, Ruben and Yang, Jimei and Hong, Seunghoon and Lin, Xunyu and Lee, Honglak},
  title     = {Decomposing motion and content for natural video sequence prediction},
  journal   = {arXiv preprint arXiv:1706.08033},
  year      = {2017},
  file      = {:villegas2017decomposing - Decomposing motion and content for natural video sequence prediction.pdf:PDF},
  groups    = {Dynamic Gan},
  owner     = {shehabk},
  timestamp = {2018.06.18},
}

@Article{lideep,
  author    = {Shan Li, Weihong Deng?},
  title     = {Deep Facial Expression Recognition: A Survey},
  file      = {:lideep - Deep Facial Expression Recognition_ A Survey.pdf:PDF},
  groups    = {paper review},
  owner     = {shehabk},
  timestamp = {2018.06.19},
}

@InProceedings{bmvc2018joint,
  author   = {BMVC 005},
  title    = {JOINT AU LOCALISATION AND INTENSITY ESTIMATION},
  year     = {2018},
  abstract = {010
011 This paper proposes a supervised learning approach to jointly perform facial Action Unit
012 (AU) localisation and intensity estimation. Contrary to previous works that try to learn},
  file     = {:bmvc2018joint - JOINT AU LOCALISATION AND INTENSITY ESTIMATION.pdf:PDF},
  groups   = {paper review},
}

@InProceedings{bradski4886learning,
  author = {Gary Bradski and Adrian Kaehler},
  title  = {Learning OpenCV},
  year   = {4886},
  file   = {:bradski4886learning - Learning OpenCV.pdf:PDF},
  groups = {opencv},
}

@Article{kaehlerpractical,
  author    = {Adrian Kaehler},
  title     = {Practical Python and OpenCV},
  file      = {:kaehlerpractical - Practical Python and OpenCV.pdf:PDF},
  groups    = {opencv},
  owner     = {shehabk},
  timestamp = {2018.07.09},
}

@Article{kaehlercase,
  author    = {Adrian Kaehler},
  title     = {Case Studies},
  file      = {:kaehlercase - Case Studies.pdf:PDF},
  groups    = {opencv},
  owner     = {shehabk},
  timestamp = {2018.07.09},
}

@Misc{minichino2015learning,
  author    = {Minichino, Joe and Howse, Joseph},
  title     = {Learning OpenCV 3 Computer Vision with Python Second Edition},
  month     = sep,
  year      = {2015},
  file      = {Joe Minichino, Joseph Howse - Learning OpenCV 3 Computer Vision with Python (2015, Packt Publishing).pdf:minichino2015learning - Learning OpenCV 3 Computer Vision with Python Second Edition.pdf:PDF},
  groups    = {opencv},
  publisher = {Packt Publishing},
}

@Misc{smithcython,
  author = {Kurt W. Smith},
  title  = {Cython: A Guide for Python Programmers},
  file   = {Cython-A-Guide-for-Python-Programmers.pdf:smithcython - Cython_ A Guide for Python Programmers.pdf:PDF},
  groups = {Python},
}

@InProceedings{Bai_2018_CVPR,
  author    = {Bai, Yancheng and Zhang, Yongqiang and Ding, Mingli and Ghanem, Bernard},
  title     = {Finding Tiny Faces in the Wild With Generative Adversarial Network},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2018},
  month     = {June},
  file      = {:Bai_2018_CVPR - Finding Tiny Faces in the Wild With Generative Adversarial Network.pdf:PDF},
  groups    = {cvpr18},
  owner     = {shehabk},
  timestamp = {2018.07.13},
}

@InProceedings{Yang_2018_CVPR,
  author    = {Yang, Hongyu and Huang, Di and Wang, Yunhong and Jain, Anil K.},
  title     = {Learning Face Age Progression: A Pyramid Architecture of GANs},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2018},
  month     = {June},
  file      = {:Yang_2018_CVPR - Learning Face Age Progression_ A Pyramid Architecture of GANs.pdf:PDF},
  groups    = {cvpr18},
  owner     = {shehabk},
  timestamp = {2018.07.13},
}

@InProceedings{Chang_2018_CVPR,
  author    = {Chang, Huiwen and Lu, Jingwan and Yu, Fisher and Finkelstein, Adam},
  title     = {PairedCycleGAN: Asymmetric Style Transfer for Applying and Removing Makeup},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2018},
  month     = {June},
  file      = {:Chang_2018_CVPR - PairedCycleGAN_ Asymmetric Style Transfer for Applying and Removing Makeup.pdf:PDF},
  groups    = {cvpr18},
  owner     = {shehabk},
  timestamp = {2018.07.13},
}

@InProceedings{Mueller_2018_CVPR,
  author    = {Mueller, Franziska and Bernard, Florian and Sotnychenko, Oleksandr and Mehta, Dushyant and Sridhar, Srinath and Casas, Dan and Theobalt, Christian},
  title     = {GANerated Hands for Real-Time 3D Hand Tracking From Monocular RGB},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2018},
  month     = {June},
  file      = {:Mueller_2018_CVPR - GANerated Hands for Real-Time 3D Hand Tracking From Monocular RGB.pdf:PDF},
  groups    = {cvpr18},
  owner     = {shehabk},
  timestamp = {2018.07.13},
}

@InProceedings{Ma_2018_CVPR,
  author    = {Ma, Liqian and Sun, Qianru and Georgoulis, Stamatios and Van Gool, Luc and Schiele, Bernt and Fritz, Mario},
  title     = {Disentangled Person Image Generation},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2018},
  month     = {June},
  file      = {:Ma_2018_CVPR - Disentangled Person Image Generation.pdf:PDF},
  groups    = {cvpr18},
  owner     = {shehabk},
  timestamp = {2018.07.13},
}

@InProceedings{Bulat_2018_CVPR,
  author    = {Bulat, Adrian and Tzimiropoulos, Georgios},
  title     = {Super-FAN: Integrated Facial Landmark Localization and Super-Resolution of Real-World Low Resolution Faces in Arbitrary Poses With GANs},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2018},
  month     = {June},
  file      = {:Bulat_2018_CVPR - Super-FAN_ Integrated Facial Landmark Localization and Super-Resolution of Real-World Low Resolution Faces in Arbitrary Poses With GANs.pdf:PDF},
  groups    = {cvpr18},
  owner     = {shehabk},
  timestamp = {2018.07.13},
}

@InProceedings{Si_2018_CVPR,
  author    = {Si, Chenyang and Wang, Wei and Wang, Liang and Tan, Tieniu},
  title     = {Multistage Adversarial Losses for Pose-Based Human Image Synthesis},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2018},
  month     = {June},
  file      = {:Si_2018_CVPR - Multistage Adversarial Losses for Pose-Based Human Image Synthesis.pdf:PDF},
  groups    = {cvpr18},
  owner     = {shehabk},
  timestamp = {2018.07.13},
}

@InProceedings{Dong_2018_CVPR,
  author    = {Dong, Xuanyi and Yu, Shoou-I and Weng, Xinshuo and Wei, Shih-En and Yang, Yi and Sheikh, Yaser},
  title     = {Supervision-by-Registration: An Unsupervised Approach to Improve the Precision of Facial Landmark Detectors},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2018},
  month     = {June},
  file      = {:Dong_2018_CVPR - Supervision-by-Registration_ An Unsupervised Approach to Improve the Precision of Facial Landmark Detectors.pdf:PDF},
  groups    = {cvpr18},
  owner     = {shehabk},
  timestamp = {2018.07.13},
}

@InProceedings{Li_2018_CVPR,
  author    = {Li, Shuang and Bak, Slawomir and Carr, Peter and Wang, Xiaogang},
  title     = {Diversity Regularized Spatiotemporal Attention for Video-Based Person Re-Identification},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2018},
  month     = {June},
  file      = {:Li_2018_CVPR - Diversity Regularized Spatiotemporal Attention for Video-Based Person Re-Identification.pdf:PDF},
  groups    = {cvpr18},
  owner     = {shehabk},
  timestamp = {2018.07.13},
}

@InProceedings{dong2018style,
  author    = {Dong, Xuanyi and Yan, Yan and Ouyang, Wanli and Yang, Yi},
  title     = {Style Aggregated Network for Facial Landmark Detection},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2018},
  month     = {June},
  file      = {:Dong_2018_CVPR - Style Aggregated Network for Facial Landmark Detection.pdf:PDF},
  groups    = {cvpr18},
  owner     = {shehabk},
  timestamp = {2018.07.13},
}

@InProceedings{Wang_2018_CVPR,
  author    = {Wang, Kang and Zhao, Rui and Ji, Qiang},
  title     = {A Hierarchical Generative Model for Eye Image Synthesis and Eye Gaze Estimation},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2018},
  month     = {June},
  file      = {:Wang_2018_CVPR - A Hierarchical Generative Model for Eye Image Synthesis and Eye Gaze Estimation.pdf:PDF},
  groups    = {cvpr18},
  owner     = {shehabk},
  timestamp = {2018.07.13},
}

@InProceedings{Baradel_2018_CVPR,
  author    = {Baradel, Fabien and Wolf, Christian and Mille, Julien and Taylor, Graham W.},
  title     = {Glimpse Clouds: Human Activity Recognition From Unstructured Feature Points},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2018},
  month     = {June},
  file      = {:Baradel_2018_CVPR - Glimpse Clouds_ Human Activity Recognition From Unstructured Feature Points.pdf:PDF},
  groups    = {cvpr18},
  owner     = {shehabk},
  timestamp = {2018.07.13},
}

@InProceedings{Zhang_2018_CVPR,
  author    = {Zhang, Xiaoning and Wang, Tiantian and Qi, Jinqing and Lu, Huchuan and Wang, Gang},
  title     = {Progressive Attention Guided Recurrent Network for Salient Object Detection},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2018},
  month     = {June},
  file      = {:Zhang_2018_CVPR - Progressive Attention Guided Recurrent Network for Salient Object Detection.pdf:PDF},
  groups    = {cvpr18},
  owner     = {shehabk},
  timestamp = {2018.07.13},
}

@InProceedings{Mopuri_2018_CVPR,
  author    = {Reddy Mopuri, Konda and Ojha, Utkarsh and Garg, Utsav and Venkatesh Babu, R.},
  title     = {NAG: Network for Adversary Generation},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2018},
  month     = {June},
  file      = {:Mopuri_2018_CVPR - NAG_ Network for Adversary Generation.pdf:PDF},
  groups    = {cvpr18},
  owner     = {shehabk},
  timestamp = {2018.07.13},
}

@InProceedings{Shen_2018_CVPR,
  author    = {Shen, Yujun and Luo, Ping and Yan, Junjie and Wang, Xiaogang and Tang, Xiaoou},
  title     = {FaceID-GAN: Learning a Symmetry Three-Player GAN for Identity-Preserving Face Synthesis},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2018},
  month     = {June},
  file      = {:Shen_2018_CVPR - FaceID-GAN_ Learning a Symmetry Three-Player GAN for Identity-Preserving Face Synthesis.pdf:PDF},
  groups    = {cvpr18},
  owner     = {shehabk},
  timestamp = {2018.07.13},
}

@InProceedings{Kossaifi_2018_CVPR,
  author    = {Kossaifi, Jean and Tran, Linh and Panagakis, Yannis and Pantic, Maja},
  title     = {GAGAN: Geometry-Aware Generative Adversarial Networks},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2018},
  month     = {June},
  file      = {:Kossaifi_2018_CVPR - GAGAN_ Geometry-Aware Generative Adversarial Networks.pdf:PDF},
  groups    = {cvpr18},
  owner     = {shehabk},
  timestamp = {2018.07.13},
}

@InProceedings{Xu_2018_CVPR,
  author    = {Xu, Tao and Zhang, Pengchuan and Huang, Qiuyuan and Zhang, Han and Gan, Zhe and Huang, Xiaolei and He, Xiaodong},
  title     = {AttnGAN: Fine-Grained Text to Image Generation With Attentional Generative Adversarial Networks},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2018},
  month     = {June},
  file      = {:Xu_2018_CVPR - AttnGAN_ Fine-Grained Text to Image Generation With Attentional Generative Adversarial Networks.pdf:PDF},
  groups    = {cvpr18},
  owner     = {shehabk},
  timestamp = {2018.07.13},
}

@Article{cai2017deep,
  author    = {Cai, Haoye and Bai, Chunyan and Tai, Yu-Wing and Tang, Chi-Keung},
  title     = {Deep Video Generation, Prediction and Completion of Human Action Sequences},
  journal   = {arXiv preprint arXiv:1711.08682},
  year      = {2017},
  comment   = {Used LSTM as discriminator. Very good paper to read.},
  file      = {:cai2017deep - Deep Video Generation, Prediction and Completion of Human Action Sequences.pdf:PDF},
  groups    = {Dynamic Gan},
  owner     = {shehabk},
  timestamp = {2018.07.16},
}

@Misc{szeliskicomputer,
  author   = {Richard Szeliski},
  title    = {Computer Vision: Algorithms and Applications},
  abstract = {Computer vision},
  file     = {Computer_Vision_SzeliskiBook_20100903_draft.pdf:szeliskicomputer - Computer Vision_ Algorithms and Applications.pdf:PDF},
  groups   = {Books},
  keywords = {cv},
}

@Misc{kollerprobabilistic,
  author   = {Daphne Koller and Nir Friedman},
  title    = {Probabilistic Graphical Models: Principles and Techniques},
  file     = {(Adaptive computation and machine learning) Daphne Koller_ Nir Friedman -Probabilistic graphical models _ principles and techniques-MIT Press (2009).pdf:kollerprobabilistic - Probabilistic Graphical Models_ Principles and Techniques.pdf:PDF},
  groups   = {Books},
  keywords = {pgm},
}

@Misc{gonzalezdigital,
  author   = {Gonzalez},
  title    = {Digital Image Processing},
  file     = {Digital_Image_Processing_GONZALEZ.pdf:gonzalezdigital - Digital Image Processing.pdf:PDF},
  groups   = {Books},
  keywords = {dip},
}

@Misc{dip3emanualmasterfiledvi,
  author   = {Gonzalez},
  title    = {DIP_3E_Manual_Master_File.dvi},
  file     = {DIP3E_Complete_Manual.pdf:/media/shehabk/D_DRIVE/study_at_usc/usc_materials/Books/DIP3E_Complete_Manual.pdf:PDF},
  groups   = {Books},
  keywords = {dip},
}

@TechReport{laakmann2005fourth,
  author   = {Gayle Laakmann},
  title    = {FOURTH CRACKING THE EDITION},
  year     = {2005},
  number   = {Questions},
  doi      = {ng},
  file     = {:laakmann2005fourth - FOURTH CRACKING THE EDITION.pdf:PDF},
  groups   = {Books},
  keywords = {ctci},
}

@Misc{xiangschaums,
  author = {By Zhigang Xiang, Roy A. Plastock},
  title  = {Schaum's outline of theory and problems of computer graphics},
  file   = {Computer_Graphics.pdf:xiangschaums - Schaum's outline of theory and problems of computer graphics.pdf:PDF},
  groups = {Books},
}

@InProceedings{learningpython,
  author  = {Machine Learning and Deep Learning and with Python and scikit-learn and TensorFlow},
  title   = {Python Machine Learning Second Edition},
  comment = {code: https://github.com/rasbt/python-machine-learning-book-2nd-edition},
  file    = {:learningpython - Python Machine Learning Second Edition.pdf:PDF},
  groups  = {Python},
}

@InProceedings{vondrick2016generating,
  author    = {Vondrick, Carl and Pirsiavash, Hamed and Torralba, Antonio},
  title     = {Generating videos with scene dynamics},
  booktitle = {Advances In Neural Information Processing Systems},
  year      = {2016},
  pages     = {613--621},
  file      = {:vondrick2016generating - Generating videos with scene dynamics.pdf:PDF},
  groups    = {Dynamic Gan},
  owner     = {shehabk},
  timestamp = {2018.07.26},
}

@Article{pumarola2018ganimation,
  author      = {Albert Pumarola and Antonio Agudo and Aleix M. Martinez and Alberto Sanfeliu and Francesc Moreno-Noguer},
  title       = {GANimation: Anatomically-aware Facial Animation from a Single Image},
  abstract    = {Recent advances in Generative Adversarial Networks (GANs) have shown impressive results for task of facial expression synthesis. The most successful architecture is StarGAN, that conditions GANs generation process with images of a specific domain, namely a set of images of persons sharing the same expression. While effective, this approach can only generate a discrete number of expressions, determined by the content of the dataset. To address this limitation, in this paper, we introduce a novel GAN conditioning scheme based on Action Units (AU) annotations, which describes in a continuous manifold the anatomical facial movements defining a human expression. Our approach allows controlling the magnitude of activation of each AU and combine several of them. Additionally, we propose a fully unsupervised strategy to train the model, that only requires images annotated with their activated AUs, and exploit attention mechanisms that make our network robust to changing backgrounds and lighting conditions. Extensive evaluation show that our approach goes beyond competing conditional generators both in the capability to synthesize a much wider range of expressions ruled by anatomically feasible muscle movements, as in the capacity of dealing with images in the wild.},
  date        = {2018-07-24},
  eprint      = {1807.09251v1},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {:pumarola2018ganimation - GANimation_ Anatomically-aware Facial Animation from a Single Image.pdf:PDF},
  groups      = {Dynamic Gan},
  keywords    = {cs.CV},
  owner       = {shehabk},
  timestamp   = {2018.07.26},
}

@Article{fan2018controllable,
  author  = {Fan, Lijie and Huang, Wenbing and Gan, Chuang and Huang, Junzhou and Gong, Boqing},
  title   = {Controllable Image-to-Video Translation: A Case Study on Facial Expression Generation},
  journal = {arXiv preprint arXiv:1808.02992},
  year    = {2018},
  file    = {:fan2018controllable - Controllable Image-to-Video Translation_ A Case Study on Facial Expression Generation.pdf:PDF},
  groups  = {Dynamic Gan},
}

@Article{du2014compound,
  author    = {Du, Shichuan and Tao, Yong and Martinez, Aleix M},
  title     = {Compound facial expressions of emotion},
  journal   = {Proceedings of the National Academy of Sciences},
  year      = {2014},
  pages     = {201322355},
  file      = {:du2014compound - Compound facial expressions of emotion.pdf:PDF},
  groups    = {Expression},
  publisher = {National Acad Sciences},
}

@InProceedings{fabian2016emotionet,
  author    = {Fabian Benitez-Quiroz, C and Srinivasan, Ramprakash and Martinez, Aleix M},
  title     = {Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2016},
  pages     = {5562--5570},
  file      = {:fabian2016emotionet - Emotionet_ An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild.pdf:PDF},
  groups    = {Expression},
}

@InProceedings{zhao2018learning,
  author    = {Zhao, Kaili and Chu, Wen-Sheng and Martinez, Aleix M},
  title     = {Learning Facial Action Units From Web Images With Scalable Weakly Supervised Clustering},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2018},
  pages     = {2090--2099},
  file      = {:zhao2018learning - Learning Facial Action Units From Web Images With Scalable Weakly Supervised Clustering.pdf:PDF},
  groups    = {AU},
}

@InProceedings{benitez2017recognition,
  author    = {Benitez-Quiroz, Carlos Fabian and Wang, Yan and Martinez, Aleix M},
  title     = {Recognition of Action Units in the Wild with Deep Nets and a New Global-Local Loss.},
  booktitle = {ICCV},
  year      = {2017},
  pages     = {3990--3999},
  file      = {:benitez2017recognition - Recognition of Action Units in the Wild with Deep Nets and a New Global-Local Loss..pdf:PDF},
  groups    = {AU},
}

@InProceedings{fabianbenitez-quiroz2016emotionet,
  author    = {Fabian Benitez-Quiroz, C and Srinivasan, Ramprakash and Martinez, Aleix M},
  title     = {Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2016},
  pages     = {5562--5570},
  file      = {:fabianbenitez-quiroz2016emotionet - Emotionet_ An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild.pdf:PDF},
  groups    = {AU},
}

@Article{,
  groups = {Mecha},
}

@Article{bansal2018recycle,
  author  = {Bansal, Aayush and Ma, Shugao and Ramanan, Deva and Sheikh, Yaser},
  title   = {Recycle-GAN: Unsupervised Video Retargeting},
  journal = {arXiv preprint arXiv:1808.05174},
  year    = {2018},
  file    = {:bansal2018recycle - Recycle-GAN_ Unsupervised Video Retargeting.pdf:PDF},
  groups  = {Dynamic Gan},
}

@Article{lee2018diverse,
  author      = {Hsin-Ying Lee and Hung-Yu Tseng and Jia-Bin Huang and Maneesh Kumar Singh and Ming-Hsuan Yang},
  title       = {Diverse Image-to-Image Translation via Disentangled Representations},
  journal     = {ECCV18},
  abstract    = {Image-to-image translation aims to learn the mapping between two visual domains. There are two main challenges for many applications: 1) the lack of aligned training pairs and 2) multiple possible outputs from a single input image. In this work, we present an approach based on disentangled representation for producing diverse outputs without paired training images. To achieve diversity, we propose to embed images onto two spaces: a domain-invariant content space capturing shared information across domains and a domain-specific attribute space. Our model takes the encoded content features extracted from a given input and the attribute vectors sampled from the attribute space to produce diverse outputs at test time. To handle unpaired training data, we introduce a novel cross-cycle consistency loss based on disentangled representations. Qualitative results show that our model can generate diverse and realistic images on a wide range of tasks without paired training data. For quantitative comparisons, we measure realism with user study and diversity with a perceptual distance metric. We apply the proposed model to domain adaptation and show competitive performance when compared to the state-of-the-art on the MNIST-M and the LineMod datasets.},
  date        = {2018-08-02},
  eprint      = {1808.00948v1},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1808.00948v1:PDF},
  groups      = {Image Translation GAN},
  keywords    = {cs.CV},
}

@Article{huang2018multimodal,
  author      = {Xun Huang and Ming-Yu Liu and Serge Belongie and Jan Kautz},
  title       = {Multimodal Unsupervised Image-to-Image Translation},
  abstract    = {Unsupervised image-to-image translation is an important and challenging problem in computer vision. Given an image in the source domain, the goal is to learn the conditional distribution of corresponding images in the target domain, without seeing any pairs of corresponding images. While this conditional distribution is inherently multimodal, existing approaches make an overly simplified assumption, modeling it as a deterministic one-to-one mapping. As a result, they fail to generate diverse outputs from a given source domain image. To address this limitation, we propose a Multimodal Unsupervised Image-to-image Translation (MUNIT) framework. We assume that the image representation can be decomposed into a content code that is domain-invariant, and a style code that captures domain-specific properties. To translate an image to another domain, we recombine its content code with a random style code sampled from the style space of the target domain. We analyze the proposed framework and establish several theoretical results. Extensive experiments with comparisons to the state-of-the-art approaches further demonstrates the advantage of the proposed framework. Moreover, our framework allows users to control the style of translation outputs by providing an example style image. Code and pretrained models are available at https://github.com/nvlabs/MUNIT},
  date        = {2018-04-12},
  eprint      = {1804.04732v2},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1804.04732v2:PDF},
  groups      = {Image Translation GAN},
  keywords    = {cs.CV, cs.LG, stat.ML},
}

@Article{braley2018group,
  author  = {Braley, Mckenzie and Murray, Gabriel},
  title   = {The Group Affect and Performance (GAP) Corpus},
  journal = {Proceedings of Group Interaction Frontiers in Technology (GIFT 2018). ACM},
  year    = {2018},
  comment = {https://sites.google.com/view/gap-corpus/home},
  doi     = {10.1145/3279981.3279985},
  file    = {:braley2018group - The Group Affect and Performance (GAP) Corpus.pdf:PDF},
  groups  = {GroupInteraction},
}

@Article{sanchez2012nonverbal,
  title     = {A nonverbal behavior approach to identify emergent leaders in small groups},
  doi       = {10.1109/tmm.2011.2181941},
  file      = {:sanchez2012nonverbal - A nonverbal behavior approach to identify emergent leaders in small groups.pdf:PDF},
  groups    = {GroupInteraction},
  publisher = {IEEE},
}

@InProceedings{laakmanncracking,
  author   = {Laakmann},
  title    = {CRACKING the Coding Interview 6th Edition},
  file     = {:laakmanncracking - CRACKING the Coding Interview 6th Edition.pdf:PDF},
  groups   = {Books},
  keywords = {ctci},
}

@InProceedings{strang8120introduction,
  author   = {Gilbert Strang},
  title    = {Introduction to Linear Algebra Fifth Edition},
  year     = {8120},
  file     = {:strang8120introduction - Introduction to Linear Algebra Fifth Edition.pdf:PDF},
  groups   = {Books},
  keywords = {linear algebra, 18.06},
}

@InProceedings{to8120introduction,
  author = {Gilbert Strang},
  title  = {Introduction to Linear Algebra Fifth Edition Solution},
  year   = {8120},
  file   = {:to8120introduction - Introduction to Linear Algebra Fifth Edition Solution.pdf:PDF},
  groups = {Books},
}

@InProceedings{zhao2018general,
  author       = {Zhao, Zijian and Wu, Xingming and Chen, Peter CY and Chen, Weihai},
  title        = {General Recurrent Attention Model for Jointly Multiple Object Recognition and Weakly Supervised Localization},
  booktitle    = {2018 25th IEEE International Conference on Image Processing (ICIP)},
  year         = {2018},
  pages        = {341--345},
  organization = {IEEE},
  file         = {:zhao2018general - General Recurrent Attention Model for Jointly Multiple Object Recognition and Weakly Supervised Localization.pdf:PDF},
  groups       = {Attention Mechanism},
}

@Article{ba2014multiple,
  author  = {Ba, Jimmy and Mnih, Volodymyr and Kavukcuoglu, Koray},
  title   = {Multiple object recognition with visual attention},
  journal = {arXiv preprint arXiv:1412.7755},
  year    = {2014},
  file    = {:/home/shehabk/Downloads/zhao2018.pdf:PDF},
  groups  = {Attention Mechanism},
}

@InProceedings{mnih2014recurrent,
  author    = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and others},
  title     = {Recurrent models of visual attention},
  booktitle = {Advances in neural information processing systems},
  year      = {2014},
  pages     = {2204--2212},
  file      = {:mnih2014recurrent - Recurrent models of visual attention.pdf:PDF},
  groups    = {Attention Mechanism},
}

@InProceedings{ablavatski2017enriched,
  author       = {Ablavatski, Artsiom and Lu, Shijian and Cai, Jianfei},
  title        = {Enriched deep recurrent visual attention model for multiple object recognition},
  booktitle    = {Applications of Computer Vision (WACV), 2017 IEEE Winter Conference on},
  year         = {2017},
  pages        = {971--978},
  organization = {IEEE},
  file         = {:ablavatski2017enriched - Enriched deep recurrent visual attention model for multiple object recognition.pdf:PDF},
  groups       = {Attention Mechanism},
}

@Article{jianjunpattern,
  author = {Jianjun, Hu},
  title  = {Pattern Recognition 768},
  file   = {:jianjunpattern - Pattern Recognition 768.pdf:PDF},
  groups = {Course Slides},
}

@Article{silbersatzoperating,
  author = {Silbersatz},
  title  = {Operating Systems},
  file   = {:silbersatzoperating - Operating Systems.pdf:PDF},
  groups = {Course Slides},
}

@Book{burkovhundred,
  title  = {Hundred Page Machine Learning},
  author = {Andriy Burkov},
  file   = {:burkovhundred - Hundred Page Machine Learning.pdf:PDF},
  groups = {Course Slides},
}

@InProceedings{khan2018group,
  author       = {Khan, Ahmed Shehab and Li, Zhiyuan and Cai, Jie and Meng, Zibo and O'Reilly, James and Tong, Yan},
  title        = {Group-Level Emotion Recognition using Deep Models with A Four-stream Hybrid Network},
  booktitle    = {Proceedings of the 2018 on International Conference on Multimodal Interaction},
  year         = {2018},
  pages        = {623--629},
  organization = {ACM},
  file         = {:khan2018group - Group-Level Emotion Recognition using Deep Models with A Four-stream Hybrid Network.pdf:PDF},
  groups       = {MyGroup},
}

@Article{alam2016transient,
  author    = {Alam, Tamanna and Khan, Ahmed Shehab and Li, Wenming and Yang, Fanghao and Tong, Yan and Khan, Jamil and Li, Chen},
  title     = {Transient force analysis and bubble dynamics during flow boiling in silicon nanowire microchannels},
  journal   = {International Journal of Heat and Mass Transfer},
  year      = {2016},
  volume    = {101},
  pages     = {937--947},
  file      = {:alam2016transient - Transient force analysis and bubble dynamics during flow boiling in silicon nanowire microchannels.pdf:PDF},
  groups    = {MyGroup},
  publisher = {Elsevier},
}

@Book{hennessy2011computer,
  title     = {Computer Architecture, Fifth Edition: A Quantitative Approach},
  publisher = {Morgan Kaufmann},
  year      = {2011},
  author    = {John L. Hennessy, David A. Patterson},
  series    = {The Morgan Kaufmann Series in Computer Architecture and Design},
  edition   = {5},
  isbn      = {012383872X,9780123838728},
  file      = {:hennessy2011computer - Computer Architecture, Fifth Edition_ A Quantitative Approach.pdf:PDF},
  groups    = {Books},
  url       = {http://gen.lib.rus.ec/book/index.php?md5=041F6D8FB8E6B6D1ED26A824775B5A0D},
}

@Article{mantoncomputer,
  author = {Mathews Manton},
  title  = {Computer Architecture},
  file   = {:mantoncomputer - Computer Architecture.pdf:PDF},
  groups = {Course Slides},
}

@Article{roseoperating,
  author = {Rose},
  title  = {Operating Systems},
  file   = {:roseoperating - Operating Systems.pdf:PDF},
  groups = {Course Slides},
}

@Book{silberschatzoperating,
  title  = {Operating System Concepts},
  author = {Silberschatz},
  file   = {:silberschatzoperating - Operating System Concepts.pdf:PDF},
  groups = {Books},
}

@TechReport{jetbrainspycharmkeymap,
  author = {jetbrains},
  title  = {pycharm_keymap},
  file   = {:jetbrainspycharmkeymap - pycharm_keymap.pdf:PDF},
  groups = {cheat_sheets},
}

@TechReport{microsoftvscodekeymap,
  author = {microsoft},
  title  = {vscode_keymap},
  file   = {:microsoftvscodekeymap - vscode_keymap.pdf:PDF},
  groups = {cheat_sheets},
}

@InProceedings{kosti2017emotion,
  author    = {Kosti, Ronak and Alvarez, Jose M and Recasens, Adria and Lapedriza, Agata},
  title     = {Emotion recognition in context},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2017},
  pages     = {1667--1675},
  file      = {:kosti2017emotion - Emotion recognition in context.pdf:PDF},
  groups    = {Emotion_in_context},
}

@Conference{survey,
  author = {Xiaoxu li},
  title  = {A Survey: Human Emotion Recognition in Context},
  file   = {:survey - A Survey_ Human Emotion Recognition in Context.pdf:PDF},
  groups = {Emotion_in_context},
}

@Article{song2018boosting,
  author    = {Song, Kaikai and Yao, Ting and Ling, Qiang and Mei, Tao},
  title     = {Boosting image sentiment analysis with visual attention},
  journal   = {Neurocomputing},
  year      = {2018},
  volume    = {312},
  pages     = {218--228},
  file      = {:song2018boosting - Boosting image sentiment analysis with visual attention.pdf:PDF},
  groups    = {Emotion_in_context},
  publisher = {Elsevier},
}

@Article{image,
  author    = {Namita Mittal and Divya Sharma and Manju Lata Joshi},
  title     = {Image Sentiment Analysis Using Deep Learning},
  year      = {2018},
  month     = {dec},
  booktitle = {2018 {IEEE}/{WIC}/{ACM} International Conference on Web Intelligence ({WI})},
  doi       = {10.1109/wi.2018.00-11},
  file      = {:image - Image Sentiment Analysis Using Deep Learning:},
  groups    = {Emotion_in_context},
  publisher = {{IEEE}},
}

@InProceedings{emotion,
  author    = {Chen Chen and Zuxuan Wu and Yu-Gang Jiang},
  title     = {Emotion in context: Deep semantic feature fusion for video emotion recognition},
  booktitle = {Proceedings of the 2016 {ACM} on Multimedia Conference - {MM} {\textquotesingle}16},
  year      = {2016},
  publisher = {{ACM} Press},
  doi       = {10.1145/2964284.2967196},
  file      = {:emotion - Emotion in context_ Deep semantic feature fusion for video emotion recognition.pdf:PDF},
  groups    = {Emotion_in_context},
}

@Article{Brosch_2010,
  author    = {Tobias Brosch and Gilles Pourtois and David Sander},
  title     = {The perception and categorisation of emotional stimuli: A review},
  journal   = {Cognition {\&} Emotion},
  year      = {2010},
  volume    = {24},
  number    = {3},
  pages     = {377--400},
  month     = {apr},
  doi       = {10.1080/02699930902975754},
  file      = {:Brosch_2010 - The perception and categorisation of emotional stimuli_ A review.pdf:PDF},
  groups    = {Emotion_in_context},
  keywords  = {psychology},
  publisher = {Informa {UK} Limited},
}

@Article{Compton_2003,
  author    = {Rebecca J. Compton},
  title     = {The interface between emotion and attention: a review of evidence from psychology and neuroscience.},
  journal   = {Behavioral and Cognitive Neuroscience Reviews},
  year      = {2003},
  volume    = {2},
  number    = {2},
  pages     = {115--129},
  month     = {jun},
  doi       = {10.1177/1534582303002002003},
  file      = {:Compton_2003 - The interface between emotion and attention_ a review of evidence from psychology and neuroscience..pdf:PDF},
  groups    = {Emotion_in_context},
  publisher = {{SAGE} Publications},
}

@Article{Campos_2017,
  author    = {V{\'{\i}}ctor Campos and Brendan Jou and Xavier Gir{\'{o}}-i-Nieto},
  title     = {From Pixels to Sentiment: Fine-tuning CNNs for Visual Sentiment Prediction},
  journal   = {Image and Vision Computing},
  year      = {2017},
  volume    = {65},
  pages     = {15--22},
  month     = {sep},
  comment   = {Simple CNN architecture based on caffenet.},
  doi       = {10.1016/j.imavis.2017.01.011},
  file      = {:Campos_2017 - From Pixels to Sentiment_ Fine-tuning CNNs for Visual Sentiment Prediction.pdf:PDF},
  groups    = {Emotion_in_scene},
  keywords  = {category},
  publisher = {Elsevier {BV}},
}

@Article{rao2016learning,
  author      = {Tianrong Rao and Min Xu and Dong Xu},
  title       = {Learning Multi-level Deep Representations for Image Emotion Classification},
  abstract    = {In this paper, we propose a new deep network that learns multi-level deep representations for image emotion classification (MldrNet). Image emotion can be recognized through image semantics, image aesthetics and low-level visual features from both global and local views. Existing image emotion classification works using hand-crafted features or deep features mainly focus on either low-level visual features or semantic-level image representations without taking all factors into consideration. The proposed MldrNet combines deep representations of different levels, i.e. image semantics, image aesthetics, and low-level visual features to effectively classify the emotion types of different kinds of images, such as abstract paintings and web images. Extensive experiments on both Internet images and abstract paintings demonstrate the proposed method outperforms the state-of-the-art methods using deep features or hand-crafted features. The proposed approach also outperforms the state-of-the-art methods with at least 6% performance improvement in terms of overall classification accuracy.},
  date        = {2016-11-22},
  eprint      = {http://arxiv.org/abs/1611.07145v2},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {:rao2016learning - Learning Multi-level Deep Representations for Image Emotion Classification.pdf:PDF},
  groups      = {Emotion_in_scene},
  keywords    = {cs.CV , category},
}

@Article{Yang_2018,
  author    = {Jufeng Yang and Dongyu She and Ming Sun and Ming-Ming Cheng and Paul L. Rosin and Liang Wang},
  title     = {Visual Sentiment Prediction Based on Automatic Discovery of Affective Regions},
  journal   = {{IEEE} Transactions on Multimedia},
  year      = {2018},
  volume    = {20},
  number    = {9},
  pages     = {2513--2525},
  month     = {sep},
  doi       = {10.1109/tmm.2018.2803520},
  file      = {:Yang_2018 - Visual Sentiment Prediction Based on Automatic Discovery of Affective Regions.pdf:PDF},
  groups    = {Emotion_in_scene},
  keywords  = {category},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{zhao2018discrete,
  author    = {Sicheng Zhao and Guiguang Ding and Yue Gao and Xin Zhao and Youbao Tang and Jungong Han and Hongxun Yao and Qingming Huang},
  title     = {Discrete Probability Distribution Prediction of Image Emotions With Shared Sparse Learning},
  journal   = {{IEEE} Transactions on Affective Computing},
  year      = {2018},
  pages     = {1--1},
  doi       = {10.1109/taffc.2018.2818685},
  file      = {:zhao2018discrete - Discrete Probability Distribution Prediction of Image Emotions With Shared Sparse Learning.pdf:PDF},
  groups    = {Emotion_in_scene},
  keywords  = {multi-categorry},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{joint,
  author    = {Jufeng Yang and Dongyu She and Ming Sun},
  title     = {Joint Image Emotion Classification and Distribution Learning via Deep Convolutional Neural Network},
  year      = {2017},
  month     = {aug},
  booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence},
  comment   = {CNN Network with two outputs, one is single label, and the other is multi label.  Most likely the multilabel is trained by cross entropy loss.},
  doi       = {10.24963/ijcai.2017/456},
  file      = {:joint - Joint Image Emotion Classification and Distribution Learning via Deep Convolutional Neural Network.pdf:PDF},
  groups    = {Emotion_in_scene},
  keywords  = {multi-categorry},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
}

@InProceedings{yang2017learning,
  author    = {Yang, Jufeng and Sun, Ming and Sun, Xiaoxiao},
  title     = {Learning visual sentiment distributions via augmented conditional probability neural network},
  booktitle = {Thirty-First AAAI Conference on Artificial Intelligence},
  year      = {2017},
  comment   = {Developed a dataset, two simple cnn implementation
},
  file      = {:learning - Learning Visual Sentiment Distributions via Augmented Conditional Probability Neural Network.pdf:PDF},
  groups    = {Emotion_in_scene},
  keywords  = {multi-categorry},
}

@InProceedings{you2016building,
  author    = {You, Quanzeng and Luo, Jiebo and Jin, Hailin and Yang, Jianchao},
  title     = {Building a large scale dataset for image emotion recognition: The fine print and the benchmark},
  booktitle = {Thirtieth AAAI Conference on Artificial Intelligence},
  year      = {2016},
  comment   = {This paper created a dataset, and then finetuned some deep learning framework on top of it.},
  file      = {:you2016building - Building a large scale dataset for image emotion recognition_ The fine print and the benchmark.pdf:PDF},
  groups    = {Emotion_in_scene},
  keywords  = {category},
}

@Proceedings{dependency,
  title     = {Dependency exploitation: a unified cnn-rnn approach for visual emotion recognition,},
  year      = {2017},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  month     = {aug},
  author    = {Xinge Zhu and Liang Li and Weigang Zhang and Tianrong Rao and Min Xu and Qingming Huang and Dong Xu},
  booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence},
  doi       = {10.24963/ijcai.2017/503},
  file      = {:dependency - Dependency exploitation_ a unified cnn-rnn approach for visual emotion recognition,.pdf:PDF},
  groups    = {Emotion_in_scene},
  keywords  = {category},
}

@InProceedings{emotional,
  author    = {Shaojing Fan and Zhiqi Shen and Ming Jiang and Bryan L. Koenig and Juan Xu and Mohan S. Kankanhalli and Qi Zhao},
  title     = {Emotional attention: A study of image sentiment and visual attention},
  booktitle = {2018 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
  year      = {2018},
  month     = {jun},
  publisher = {{IEEE}},
  doi       = {10.1109/cvpr.2018.00785},
  file      = {:emotional - Emotional attention_ A study of image sentiment and visual attention.pdf:PDF},
  groups    = {Emotion_in_scene},
  keywords  = {EMOd},
}

@Article{ain2017sentiment,
  author  = {Ain, Qurat Tul and Ali, Mubashir and Riaz, Amna and Noureen, Amna and Kamran, Muhammad and Hayat, Babar and Rehman, A},
  title   = {Sentiment analysis using deep learning techniques: a review},
  journal = {Int J Adv Comput Sci Appl},
  year    = {2017},
  volume  = {8},
  number  = {6},
  pages   = {424},
  file    = {:ain2017sentiment - Sentiment analysis using deep learning techniques_ a review.pdf:PDF},
  groups  = {Emotion_in_scene},
}

@Article{Zhao_2018,
  author    = {Sicheng Zhao and Hongxun Yao and Yue Gao and Guiguang Ding and Tat-Seng Chua},
  title     = {Predicting Personalized Image Emotion Perceptions in Social Networks},
  journal   = {{IEEE} Transactions on Affective Computing},
  year      = {2018},
  volume    = {9},
  number    = {4},
  pages     = {526--540},
  month     = {oct},
  doi       = {10.1109/taffc.2016.2628787},
  file      = {:Zhao_2018 - Predicting Personalized Image Emotion Perceptions in Social Networks.pdf:PDF},
  groups    = {Emotion_in_scene},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{emotiongan,
  title  = {EmotionGAN: Unsupervised Domain Adaptation for Learning Discrete Probability Distributions of Image Emotions},
  groups = {Emotion_in_scene},
}

@Article{li2012context,
  author    = {Bing Li and Weihua Xiong and Weiming Hu and Xinmiao Ding},
  title     = {Context-aware affective images classification based on bilayer sparse representation},
  year      = {2012},
  booktitle = {Proceedings of the 20th {ACM} international conference on Multimedia - {MM} {\textquotesingle}12},
  doi       = {10.1145/2393347.2396296},
  file      = {:li2012context - Context-aware affective images classification based on bilayer sparse representation.pdf:PDF},
  groups    = {Emotion_in_scene},
  publisher = {{ACM} Press},
}

@InProceedings{coelho1992alchemist,
  author = {Paulo Coelho},
  title  = {The Alchemist},
  year   = {1992},
  file   = {:coelho1992alchemist - The Alchemist.pdf:PDF},
  groups = {Self_Help},
}

@Article{quach2018non,
  author      = {Kha Gia Quach and Ngan Le and Khoa Luu and Chi Nhan Duong and Ibsa Jalata and Karl Ricanek},
  title       = {Non-Volume Preserving-based Feature Fusion Approach to Group-Level Expression Recognition on Crowd Videos},
  abstract    = {Group-level emotion recognition (ER) is a growing research area as the demands for assessing crowds of all sizes is becoming an interest in both the security arena and social media. This work investigates group-level expression recognition on crowd videos where information is not only aggregated across a variable length sequence of frames but also over the set of faces within each frame to produce aggregated recognition results. In this paper, we propose an effective deep feature level fusion mechanism to model the spatial-temporal information in the crowd videos. Furthermore, we extend our proposed NVP fusion mechanism to temporal NVP fussion appoarch to learn the temporal information between frames. In order to demonstrate the robustness and effectiveness of each component in the proposed approach, three experiments were conducted: (i) evaluation on the AffectNet database to benchmark the proposed emoNet for recognizing facial expression; (ii) evaluation on EmotiW2018 to benchmark the proposed deep feature level fusion mechanism NVPF; and, (iii) examine the proposed TNVPF on an innovative Group-level Emotion on Crowd Videos (GECV) dataset composed of 627 videos collected from social media. GECV dataset is a collection of videos ranging in duration from 10 to 20 seconds of crowds of twenty (20) or more subjects and each video is labeled as positive, negative, or neutral.},
  date        = {2018-11-28},
  eprint      = {http://arxiv.org/abs/1811.11849v1},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1811.11849v1:PDF},
  groups      = {Emotion_in_context},
  keywords    = {cs.CV},
}

@Article{quach2018nona,
  author        = {Kha Gia Quach and Ngan Le and Khoa Luu and Chi Nhan Duong and Ibsa Jalata and Karl Ricanek},
  title         = {Non-Volume Preserving-based Feature Fusion Approach to Group-Level Expression Recognition on Crowd Videos},
  __markedentry = {[shehabk:6]},
  abstract      = {Group-level emotion recognition (ER) is a growing research area as the demands for assessing crowds of all sizes is becoming an interest in both the security arena and social media. This work investigates group-level expression recognition on crowd videos where information is not only aggregated across a variable length sequence of frames but also over the set of faces within each frame to produce aggregated recognition results. In this paper, we propose an effective deep feature level fusion mechanism to model the spatial-temporal information in the crowd videos. Furthermore, we extend our proposed NVP fusion mechanism to temporal NVP fussion appoarch to learn the temporal information between frames. In order to demonstrate the robustness and effectiveness of each component in the proposed approach, three experiments were conducted: (i) evaluation on the AffectNet database to benchmark the proposed emoNet for recognizing facial expression; (ii) evaluation on EmotiW2018 to benchmark the proposed deep feature level fusion mechanism NVPF; and, (iii) examine the proposed TNVPF on an innovative Group-level Emotion on Crowd Videos (GECV) dataset composed of 627 videos collected from social media. GECV dataset is a collection of videos ranging in duration from 10 to 20 seconds of crowds of twenty (20) or more subjects and each video is labeled as positive, negative, or neutral.},
  date          = {2018-11-28},
  eprint        = {http://arxiv.org/abs/1811.11849v1},
  eprintclass   = {cs.CV},
  eprinttype    = {arXiv},
  file          = {:quach2018nona - Non-Volume Preserving-based Feature Fusion Approach to Group-Level Expression Recognition on Crowd Videos.pdf:PDF},
  groups        = {Emotion_in_context},
  keywords      = {cs.CV},
}

@Article{ghosh2018predicting,
  author      = {Shreya Ghosh and Abhinav Dhall and Nicu Sebe and Tom Gedeon},
  title       = {Predicting Group Cohesiveness in Images},
  abstract    = {The cohesiveness of a group is an essential indicator of the emotional state, structure and success of a group of people. We study the factors that influence the perception of group-level cohesion and propose methods for estimating the human-perceived cohesion on the group cohesiveness scale. In order to identify the visual cues (attributes) for cohesion, we conducted a user survey. Image analysis is performed at a group-level via a multi-task convolutional neural network. For analyzing the contribution of facial expressions of the group members for predicting the Group Cohesion Score (GCS), a capsule network is explored. We add GCS to the Group Affect database and propose the `GAF-Cohesion database'. The proposed model performs well on the database and is able to achieve near human-level performance in predicting a group's cohesion score. It is interesting to note that group cohesion as an attribute, when jointly trained for group-level emotion prediction, helps in increasing the performance for the later task. This suggests that group-level emotion and cohesion are correlated.},
  date        = {2018-12-31},
  eprint      = {http://arxiv.org/abs/1812.11771v4},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {:ghosh2018predicting - Predicting Group Cohesiveness in Images.pdf:PDF},
  groups      = {EmotiW},
  keywords    = {baseline 2019},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:AU\;0\;1\;\;\;\;;
1 StaticGroup:MyGroup\;0\;1\;\;\;\;;
1 StaticGroup:Graph CNN\;0\;1\;\;\;\;;
1 StaticGroup:Emotion_in_context\;0\;1\;\;\;\;;
1 StaticGroup:Emotion_in_scene\;0\;1\;\;\;\;;
1 StaticGroup:Expression\;0\;1\;\;\;\;;
1 StaticGroup:Generative Models\;0\;0\;\;\;\;;
2 StaticGroup:Gan Papers\;0\;1\;\;\;\;;
3 StaticGroup:Image Translation GAN\;0\;1\;\;\;\;;
3 StaticGroup:Face Synthesis GAN\;0\;1\;\;\;\;;
3 StaticGroup:Tutorials GAN\;0\;1\;\;\;\;;
3 StaticGroup:Theory GAN\;0\;1\;\;\;\;;
3 StaticGroup:Conditional Adversarial\;0\;1\;\;\;\;;
3 StaticGroup:Inpainting\;0\;1\;\;\;\;;
3 StaticGroup:High Quality Image Generation\;0\;1\;\;\;\;;
3 StaticGroup:Unlclassified\;0\;1\;\;\;\;;
1 StaticGroup:Books\;0\;1\;\;\;\;;
1 StaticGroup:paper review\;0\;1\;\;\;\;;
1 StaticGroup:DL Advancement\;0\;1\;\;\;\;;
1 StaticGroup:CNN Classic\;0\;1\;\;\;\;;
1 StaticGroup:Course Slides\;0\;1\;\;\;\;;
1 StaticGroup:EmotiW\;0\;1\;\;\;\;;
1 StaticGroup:segmentation\;0\;1\;\;\;\;;
1 StaticGroup:Scene\;0\;1\;\;\;\;;
1 StaticGroup:Python\;0\;1\;\;\;\;;
1 StaticGroup:Design Patterns and Software Arhitecture\;0\;1\;\;\;\;;
1 StaticGroup:Dynamic Gan\;0\;1\;\;\;\;;
1 StaticGroup:opencv\;0\;1\;\;\;\;;
1 StaticGroup:conferences\;0\;1\;\;\;\;;
2 StaticGroup:cvpr18\;0\;1\;\;\;\;;
2 StaticGroup:icmi2018\;0\;1\;\;\;\;;
1 StaticGroup:Mecha\;0\;1\;\;\;\;;
1 StaticGroup:GroupInteraction\;0\;1\;\;\;\;;
1 StaticGroup:Interview\;0\;1\;\;\;\;;
1 StaticGroup:Attention Mechanism\;0\;1\;\;\;\;;
1 StaticGroup:Human Interaction\;0\;1\;\;\;\;;
1 StaticGroup:cheat_sheets\;0\;1\;\;\;\;;
1 StaticGroup:Self_Help\;0\;1\;\;\;\;;
}
